iteration: 300000, loss fn: 0.10623179376125336
iteration: 300001, loss fn: 0.06761421263217926
iteration: 300002, loss fn: 0.05824216827750206
iteration: 300003, loss fn: 0.07309207320213318
iteration: 300004, loss fn: 0.09187109768390656
iteration: 300005, loss fn: 0.059878211468458176
iteration: 300006, loss fn: 0.05335702747106552
iteration: 300007, loss fn: 0.0762653797864914
iteration: 300008, loss fn: 0.04142875224351883
iteration: 300009, loss fn: 0.05533929914236069
iteration: 300010, loss fn: 0.04472366347908974
iteration: 300011, loss fn: 0.06915129721164703
iteration: 300012, loss fn: 0.05610871687531471
iteration: 300013, loss fn: 0.03362684324383736
iteration: 300014, loss fn: 0.024455269798636436
iteration: 300015, loss fn: 0.049257952719926834
iteration: 300016, loss fn: 0.0652022510766983
iteration: 300017, loss fn: 0.062308359891176224
iteration: 300018, loss fn: 0.07170679420232773
iteration: 300019, loss fn: 0.05181313678622246
iteration: 300020, loss fn: 0.06056264042854309
iteration: 300021, loss fn: 0.10142028331756592
iteration: 300022, loss fn: 0.04894651845097542
iteration: 300023, loss fn: 0.04416361078619957
iteration: 300024, loss fn: 0.08072808384895325
iteration: 300025, loss fn: 0.050790876150131226
iteration: 300026, loss fn: 0.03340611606836319
iteration: 300027, loss fn: 0.05865047499537468
iteration: 300028, loss fn: 0.027646156027913094
iteration: 300029, loss fn: 0.06533601135015488
iteration: 300030, loss fn: 0.0731215626001358
iteration: 300031, loss fn: 0.05880874767899513
iteration: 300032, loss fn: 0.09279558807611465
iteration: 300033, loss fn: 0.09307239204645157
iteration: 300034, loss fn: 0.053306907415390015
iteration: 300035, loss fn: 0.07420968264341354
iteration: 300036, loss fn: 0.02870454080402851
iteration: 300037, loss fn: 0.060176435858011246
iteration: 300038, loss fn: 0.045452166348695755
iteration: 300039, loss fn: 0.07264692336320877
iteration: 300040, loss fn: 0.016231896355748177
iteration: 300041, loss fn: 0.05918188393115997
iteration: 300042, loss fn: 0.060896918177604675
iteration: 300043, loss fn: 0.051970936357975006
iteration: 300044, loss fn: 0.05728248134255409
iteration: 300045, loss fn: 0.0922686830163002
iteration: 300046, loss fn: 0.06749627739191055
iteration: 300047, loss fn: 0.06884828209877014
iteration: 300048, loss fn: 0.08533724397420883
iteration: 300049, loss fn: 0.04634520784020424
iteration: 300050, loss fn: 0.05559750646352768
iteration: 300051, loss fn: 0.03626403957605362
iteration: 300052, loss fn: 0.044060397893190384
iteration: 300053, loss fn: 0.036093614995479584
iteration: 300054, loss fn: 0.05751877278089523
iteration: 300055, loss fn: 0.053659867495298386
iteration: 300056, loss fn: 0.07067202031612396
iteration: 300057, loss fn: 0.0796179473400116
iteration: 300058, loss fn: 0.1268935203552246
iteration: 300059, loss fn: 0.04044020175933838
iteration: 300060, loss fn: 0.07471446692943573
iteration: 300061, loss fn: 0.05075162649154663
iteration: 300062, loss fn: 0.0570945180952549
iteration: 300063, loss fn: 0.05235935375094414
iteration: 300064, loss fn: 0.059753552079200745
iteration: 300065, loss fn: 0.06808050721883774
iteration: 300066, loss fn: 0.030932463705539703
iteration: 300067, loss fn: 0.06719329953193665
iteration: 300068, loss fn: 0.07870077341794968
iteration: 300069, loss fn: 0.1027679294347763
iteration: 300070, loss fn: 0.04164056107401848
iteration: 300071, loss fn: 0.06364073604345322
iteration: 300072, loss fn: 0.027284227311611176
iteration: 300073, loss fn: 0.06295403093099594
iteration: 300074, loss fn: 0.04177246242761612
iteration: 300075, loss fn: 0.08315784484148026
iteration: 300076, loss fn: 0.029959311708807945
iteration: 300077, loss fn: 0.05097052454948425
iteration: 300078, loss fn: 0.05693424120545387
iteration: 300079, loss fn: 0.061548054218292236
iteration: 300080, loss fn: 0.05752657353878021
iteration: 300081, loss fn: 0.06366065889596939
iteration: 300082, loss fn: 0.060669053345918655
iteration: 300083, loss fn: 0.052601948380470276
iteration: 300084, loss fn: 0.0856066420674324
iteration: 300085, loss fn: 0.049691092222929
iteration: 300086, loss fn: 0.04819021373987198
iteration: 300087, loss fn: 0.05877802520990372
iteration: 300088, loss fn: 0.037119776010513306
iteration: 300089, loss fn: 0.03829265013337135
iteration: 300090, loss fn: 0.060215432196855545
iteration: 300091, loss fn: 0.05793171748518944
iteration: 300092, loss fn: 0.061624303460121155
iteration: 300093, loss fn: 0.06060732901096344
iteration: 300094, loss fn: 0.08172686398029327
iteration: 300095, loss fn: 0.05602283775806427
iteration: 300096, loss fn: 0.10082779079675674
iteration: 300097, loss fn: 0.04262528568506241
iteration: 300098, loss fn: 0.08052647113800049
iteration: 300099, loss fn: 0.03052934817969799
iteration: 300100, loss fn: 0.07640064507722855
iteration: 300101, loss fn: 0.06822433322668076
iteration: 300102, loss fn: 0.05014686658978462
iteration: 300103, loss fn: 0.08375106006860733
iteration: 300104, loss fn: 0.05431993305683136
iteration: 300105, loss fn: 0.04565659537911415
iteration: 300106, loss fn: 0.1258772909641266
iteration: 300107, loss fn: 0.05365225672721863
iteration: 300108, loss fn: 0.04807871952652931
iteration: 300109, loss fn: 0.05267009884119034
iteration: 300110, loss fn: 0.04163740947842598
iteration: 300111, loss fn: 0.05488255247473717
iteration: 300112, loss fn: 0.023743361234664917
iteration: 300113, loss fn: 0.08060763776302338
iteration: 300114, loss fn: 0.0551079623401165
iteration: 300115, loss fn: 0.08357362449169159
iteration: 300116, loss fn: 0.09688454866409302
iteration: 300117, loss fn: 0.053642407059669495
iteration: 300118, loss fn: 0.04141438379883766
iteration: 300119, loss fn: 0.036845993250608444
iteration: 300120, loss fn: 0.07009875029325485
iteration: 300121, loss fn: 0.07023276388645172
iteration: 300122, loss fn: 0.0725705549120903
iteration: 300123, loss fn: 0.05227681249380112
iteration: 300124, loss fn: 0.06560703366994858
iteration: 300125, loss fn: 0.0630883052945137
iteration: 300126, loss fn: 0.09583182632923126
iteration: 300127, loss fn: 0.05151990428566933
iteration: 300128, loss fn: 0.030963284894824028
iteration: 300129, loss fn: 0.052939802408218384
iteration: 300130, loss fn: 0.06684525310993195
iteration: 300131, loss fn: 0.03684287518262863
iteration: 300132, loss fn: 0.062281202524900436
iteration: 300133, loss fn: 0.06884761899709702
iteration: 300134, loss fn: 0.037253428250551224
iteration: 300135, loss fn: 0.04337364062666893
iteration: 300136, loss fn: 0.04020794853568077
iteration: 300137, loss fn: 0.0806395635008812
iteration: 300138, loss fn: 0.0825052559375763
iteration: 300139, loss fn: 0.049540042877197266
iteration: 300140, loss fn: 0.04275011643767357
iteration: 300141, loss fn: 0.06472795456647873
iteration: 300142, loss fn: 0.044906873255968094
iteration: 300143, loss fn: 0.03215727210044861
iteration: 300144, loss fn: 0.050652652978897095
iteration: 300145, loss fn: 0.05693629011511803
iteration: 300146, loss fn: 0.03546294942498207
iteration: 300147, loss fn: 0.03327619656920433
iteration: 300148, loss fn: 0.03438630700111389
iteration: 300149, loss fn: 0.0694236233830452
iteration: 300150, loss fn: 0.04332182928919792
iteration: 300151, loss fn: 0.079191192984581
iteration: 300152, loss fn: 0.06341741979122162
iteration: 300153, loss fn: 0.07457786053419113
iteration: 300154, loss fn: 0.049236007034778595
iteration: 300155, loss fn: 0.04384321719408035
iteration: 300156, loss fn: 0.055443450808525085
iteration: 300157, loss fn: 0.015921516343951225
iteration: 300158, loss fn: 0.1017475426197052
iteration: 300159, loss fn: 0.054422374814748764
iteration: 300160, loss fn: 0.03016410768032074
iteration: 300161, loss fn: 0.04301793873310089
iteration: 300162, loss fn: 0.04923069104552269
iteration: 300163, loss fn: 0.09512770175933838
iteration: 300164, loss fn: 0.05892658978700638
iteration: 300165, loss fn: 0.051622193306684494
iteration: 300166, loss fn: 0.07032988220453262
iteration: 300167, loss fn: 0.08016564697027206
iteration: 300168, loss fn: 0.0867798775434494
iteration: 300169, loss fn: 0.05590253323316574
iteration: 300170, loss fn: 0.0709146037697792
iteration: 300171, loss fn: 0.04341172054409981
iteration: 300172, loss fn: 0.03929430991411209
iteration: 300173, loss fn: 0.05399760603904724
iteration: 300174, loss fn: 0.05613521486520767
iteration: 300175, loss fn: 0.038213420659303665
iteration: 300176, loss fn: 0.08371227979660034
iteration: 300177, loss fn: 0.06757485866546631
iteration: 300178, loss fn: 0.06694448739290237
iteration: 300179, loss fn: 0.05764530599117279
iteration: 300180, loss fn: 0.06399887800216675
iteration: 300181, loss fn: 0.13705173134803772
iteration: 300182, loss fn: 0.04177543520927429
iteration: 300183, loss fn: 0.04300738871097565
iteration: 300184, loss fn: 0.025919459760189056
iteration: 300185, loss fn: 0.12892495095729828
iteration: 300186, loss fn: 0.04111352935433388
iteration: 300187, loss fn: 0.06702496111392975
iteration: 300188, loss fn: 0.05511387437582016
iteration: 300189, loss fn: 0.06745126843452454
iteration: 300190, loss fn: 0.07654085755348206
iteration: 300191, loss fn: 0.06026117131114006
iteration: 300192, loss fn: 0.05466257408261299
iteration: 300193, loss fn: 0.04818030074238777
iteration: 300194, loss fn: 0.0846254900097847
iteration: 300195, loss fn: 0.026155482977628708
iteration: 300196, loss fn: 0.04133182764053345
iteration: 300197, loss fn: 0.06841064244508743
iteration: 300198, loss fn: 0.03548022732138634
iteration: 300199, loss fn: 0.10582324862480164
iteration: 300200, loss fn: 0.08339974284172058
iteration: 300201, loss fn: 0.041220735758543015
iteration: 300202, loss fn: 0.05639960989356041
iteration: 300203, loss fn: 0.09818194061517715
iteration: 300204, loss fn: 0.06250956654548645
iteration: 300205, loss fn: 0.03404034301638603
iteration: 300206, loss fn: 0.05450243502855301
iteration: 300207, loss fn: 0.1259099692106247
iteration: 300208, loss fn: 0.07832805812358856
iteration: 300209, loss fn: 0.05986529588699341
iteration: 300210, loss fn: 0.0666915699839592
iteration: 300211, loss fn: 0.06242328882217407
iteration: 300212, loss fn: 0.10385148227214813
iteration: 300213, loss fn: 0.06414798647165298
iteration: 300214, loss fn: 0.060305651277303696
iteration: 300215, loss fn: 0.09272236377000809
iteration: 300216, loss fn: 0.04008083418011665
iteration: 300217, loss fn: 0.06488162279129028
iteration: 300218, loss fn: 0.09393331408500671
iteration: 300219, loss fn: 0.07564938068389893
iteration: 300220, loss fn: 0.04254419356584549
iteration: 300221, loss fn: 0.04387126490473747
iteration: 300222, loss fn: 0.0533660352230072
iteration: 300223, loss fn: 0.06912766396999359
iteration: 300224, loss fn: 0.04132390022277832
iteration: 300225, loss fn: 0.046516284346580505
iteration: 300226, loss fn: 0.050464484840631485
iteration: 300227, loss fn: 0.041598688811063766
iteration: 300228, loss fn: 0.08633706718683243
iteration: 300229, loss fn: 0.11194085329771042
iteration: 300230, loss fn: 0.05721040815114975
iteration: 300231, loss fn: 0.04475799575448036
iteration: 300232, loss fn: 0.06569091230630875
iteration: 300233, loss fn: 0.09460415691137314
iteration: 300234, loss fn: 0.035938091576099396
iteration: 300235, loss fn: 0.08090583980083466
iteration: 300236, loss fn: 0.04375721514225006
iteration: 300237, loss fn: 0.08317998051643372
iteration: 300238, loss fn: 0.08416536450386047
iteration: 300239, loss fn: 0.03414752334356308
iteration: 300240, loss fn: 0.03239179402589798
iteration: 300241, loss fn: 0.068386010825634
iteration: 300242, loss fn: 0.07068177312612534
iteration: 300243, loss fn: 0.04142308980226517
iteration: 300244, loss fn: 0.04318241402506828
iteration: 300245, loss fn: 0.03414125367999077
iteration: 300246, loss fn: 0.08820599317550659
iteration: 300247, loss fn: 0.05471257492899895
iteration: 300248, loss fn: 0.12102924287319183
iteration: 300249, loss fn: 0.025886964052915573
iteration: 300250, loss fn: 0.04597671702504158
iteration: 300251, loss fn: 0.033733487129211426
iteration: 300252, loss fn: 0.05856503173708916
iteration: 300253, loss fn: 0.028015777468681335
iteration: 300254, loss fn: 0.08954530209302902
iteration: 300255, loss fn: 0.03605266660451889
iteration: 300256, loss fn: 0.07681887596845627
iteration: 300257, loss fn: 0.07525958120822906
iteration: 300258, loss fn: 0.0640103667974472
iteration: 300259, loss fn: 0.09706903249025345
iteration: 300260, loss fn: 0.06389249116182327
iteration: 300261, loss fn: 0.08647556602954865
iteration: 300262, loss fn: 0.06266982108354568
iteration: 300263, loss fn: 0.026157816872000694
iteration: 300264, loss fn: 0.03345052897930145
iteration: 300265, loss fn: 0.05710046365857124
iteration: 300266, loss fn: 0.08345882594585419
iteration: 300267, loss fn: 0.07212065905332565
iteration: 300268, loss fn: 0.08876600116491318
iteration: 300269, loss fn: 0.05156208574771881
iteration: 300270, loss fn: 0.05626677721738815
iteration: 300271, loss fn: 0.026520272716879845
iteration: 300272, loss fn: 0.0966876819729805
iteration: 300273, loss fn: 0.06144436076283455
iteration: 300274, loss fn: 0.05771348252892494
iteration: 300275, loss fn: 0.06296634674072266
iteration: 300276, loss fn: 0.06098414584994316
iteration: 300277, loss fn: 0.04087870195508003
iteration: 300278, loss fn: 0.053040314465761185
iteration: 300279, loss fn: 0.07195251435041428
iteration: 300280, loss fn: 0.06194724887609482
iteration: 300281, loss fn: 0.04968797042965889
iteration: 300282, loss fn: 0.056527867913246155
iteration: 300283, loss fn: 0.05888567119836807
iteration: 300284, loss fn: 0.039055924862623215
iteration: 300285, loss fn: 0.09253227710723877
iteration: 300286, loss fn: 0.07289686053991318
iteration: 300287, loss fn: 0.06672783941030502
iteration: 300288, loss fn: 0.04307145997881889
iteration: 300289, loss fn: 0.03147616982460022
iteration: 300290, loss fn: 0.04143167659640312
iteration: 300291, loss fn: 0.09900493919849396
iteration: 300292, loss fn: 0.049088530242443085
iteration: 300293, loss fn: 0.0535491444170475
iteration: 300294, loss fn: 0.08124403655529022
iteration: 300295, loss fn: 0.08753884583711624
iteration: 300296, loss fn: 0.055094730108976364
iteration: 300297, loss fn: 0.08821550011634827
iteration: 300298, loss fn: 0.02472059242427349
iteration: 300299, loss fn: 0.05226799473166466
iteration: 300300, loss fn: 0.06860063970088959
iteration: 300301, loss fn: 0.0641755685210228
iteration: 300302, loss fn: 0.07321902364492416
iteration: 300303, loss fn: 0.03410815820097923
iteration: 300304, loss fn: 0.0473722442984581
iteration: 300305, loss fn: 0.045376017689704895
iteration: 300306, loss fn: 0.04127418249845505
iteration: 300307, loss fn: 0.08627071976661682
iteration: 300308, loss fn: 0.04642171040177345
iteration: 300309, loss fn: 0.07483965158462524
iteration: 300310, loss fn: 0.03692128136754036
iteration: 300311, loss fn: 0.07300004363059998
iteration: 300312, loss fn: 0.09429486840963364
iteration: 300313, loss fn: 0.01928064599633217
iteration: 300314, loss fn: 0.054825376719236374
iteration: 300315, loss fn: 0.06456346064805984
iteration: 300316, loss fn: 0.09602908045053482
iteration: 300317, loss fn: 0.021463483572006226
iteration: 300318, loss fn: 0.06391361355781555
iteration: 300319, loss fn: 0.06825912743806839
iteration: 300320, loss fn: 0.032115738838911057
iteration: 300321, loss fn: 0.08377650380134583
iteration: 300322, loss fn: 0.06370963901281357
iteration: 300323, loss fn: 0.05322727933526039
iteration: 300324, loss fn: 0.03517023101449013
iteration: 300325, loss fn: 0.05962219834327698
iteration: 300326, loss fn: 0.0627756118774414
iteration: 300327, loss fn: 0.052754856646060944
iteration: 300328, loss fn: 0.04858075827360153
iteration: 300329, loss fn: 0.030340559780597687
iteration: 300330, loss fn: 0.03098759427666664
iteration: 300331, loss fn: 0.06513872742652893
iteration: 300332, loss fn: 0.0287893358618021
iteration: 300333, loss fn: 0.06348245590925217
iteration: 300334, loss fn: 0.08298567682504654
iteration: 300335, loss fn: 0.05275210738182068
iteration: 300336, loss fn: 0.09660109132528305
iteration: 300337, loss fn: 0.05486198514699936
iteration: 300338, loss fn: 0.05634221434593201
iteration: 300339, loss fn: 0.06398437172174454
iteration: 300340, loss fn: 0.04213340952992439
iteration: 300341, loss fn: 0.03901815414428711
iteration: 300342, loss fn: 0.06793282926082611
iteration: 300343, loss fn: 0.060991618782281876
iteration: 300344, loss fn: 0.04236704856157303
iteration: 300345, loss fn: 0.05416800081729889
iteration: 300346, loss fn: 0.02442053146660328
iteration: 300347, loss fn: 0.08908133953809738
iteration: 300348, loss fn: 0.050498709082603455
iteration: 300349, loss fn: 0.07312005758285522
iteration: 300350, loss fn: 0.060707222670316696
iteration: 300351, loss fn: 0.07242301851511002
iteration: 300352, loss fn: 0.05479991063475609
iteration: 300353, loss fn: 0.061945293098688126
iteration: 300354, loss fn: 0.039582058787345886
iteration: 300355, loss fn: 0.08629290759563446
iteration: 300356, loss fn: 0.057569801807403564
iteration: 300357, loss fn: 0.08696732670068741
iteration: 300358, loss fn: 0.08389940112829208
iteration: 300359, loss fn: 0.03525539115071297
iteration: 300360, loss fn: 0.06999775022268295
iteration: 300361, loss fn: 0.04220851883292198
iteration: 300362, loss fn: 0.08223436772823334
iteration: 300363, loss fn: 0.060708187520504
iteration: 300364, loss fn: 0.09031350910663605
iteration: 300365, loss fn: 0.08131326735019684
iteration: 300366, loss fn: 0.04436611756682396
iteration: 300367, loss fn: 0.07603316754102707
iteration: 300368, loss fn: 0.0858294814825058
iteration: 300369, loss fn: 0.0683089941740036
iteration: 300370, loss fn: 0.06062791496515274
iteration: 300371, loss fn: 0.03632509335875511
iteration: 300372, loss fn: 0.029091523960232735
iteration: 300373, loss fn: 0.04219100996851921
iteration: 300374, loss fn: 0.041509177535772324
iteration: 300375, loss fn: 0.046190470457077026
iteration: 300376, loss fn: 0.06446132063865662
iteration: 300377, loss fn: 0.041390758007764816
iteration: 300378, loss fn: 0.10471145063638687
iteration: 300379, loss fn: 0.07640132308006287
iteration: 300380, loss fn: 0.07069459557533264
iteration: 300381, loss fn: 0.05068773031234741
iteration: 300382, loss fn: 0.02574346400797367
iteration: 300383, loss fn: 0.04758060351014137
iteration: 300384, loss fn: 0.06582961231470108
iteration: 300385, loss fn: 0.0621228963136673
iteration: 300386, loss fn: 0.049908678978681564
iteration: 300387, loss fn: 0.04026050865650177
iteration: 300388, loss fn: 0.04415968060493469
iteration: 300389, loss fn: 0.03403531387448311
iteration: 300390, loss fn: 0.060004912316799164
iteration: 300391, loss fn: 0.046244166791439056
iteration: 300392, loss fn: 0.06386289745569229
iteration: 300393, loss fn: 0.06550844758749008
iteration: 300394, loss fn: 0.0556696280837059
iteration: 300395, loss fn: 0.05749357119202614
iteration: 300396, loss fn: 0.051346659660339355
iteration: 300397, loss fn: 0.03758915141224861
iteration: 300398, loss fn: 0.03306938335299492
iteration: 300399, loss fn: 0.024426503106951714
iteration: 300400, loss fn: 0.09559035301208496
iteration: 300401, loss fn: 0.04567822813987732
iteration: 300402, loss fn: 0.0651940256357193
iteration: 300403, loss fn: 0.038812555372714996
iteration: 300404, loss fn: 0.03382782265543938
iteration: 300405, loss fn: 0.0537935234606266
iteration: 300406, loss fn: 0.0723043829202652
iteration: 300407, loss fn: 0.052632927894592285
iteration: 300408, loss fn: 0.058288026601076126
iteration: 300409, loss fn: 0.08301474899053574
iteration: 300410, loss fn: 0.054848797619342804
iteration: 300411, loss fn: 0.04874024912714958
iteration: 300412, loss fn: 0.03555186837911606
iteration: 300413, loss fn: 0.055703792721033096
iteration: 300414, loss fn: 0.0514652356505394
iteration: 300415, loss fn: 0.08113111555576324
iteration: 300416, loss fn: 0.07959400117397308
iteration: 300417, loss fn: 0.039825234562158585
iteration: 300418, loss fn: 0.0658649131655693
iteration: 300419, loss fn: 0.06595148146152496
iteration: 300420, loss fn: 0.07033536583185196
iteration: 300421, loss fn: 0.07043109834194183
iteration: 300422, loss fn: 0.057131439447402954
iteration: 300423, loss fn: 0.05903514847159386
iteration: 300424, loss fn: 0.06342687457799911
iteration: 300425, loss fn: 0.046886738389730453
iteration: 300426, loss fn: 0.05196095257997513
iteration: 300427, loss fn: 0.050605542957782745
iteration: 300428, loss fn: 0.06722664833068848
iteration: 300429, loss fn: 0.06182413920760155
iteration: 300430, loss fn: 0.043830711394548416
iteration: 300431, loss fn: 0.03772754967212677
iteration: 300432, loss fn: 0.04651116579771042
iteration: 300433, loss fn: 0.08889114111661911
iteration: 300434, loss fn: 0.07190942764282227
iteration: 300435, loss fn: 0.046902503818273544
iteration: 300436, loss fn: 0.02554897777736187
iteration: 300437, loss fn: 0.0599406361579895
iteration: 300438, loss fn: 0.058987416326999664
iteration: 300439, loss fn: 0.05138571932911873
iteration: 300440, loss fn: 0.04582342132925987
iteration: 300441, loss fn: 0.0497678779065609
iteration: 300442, loss fn: 0.05316486209630966
iteration: 300443, loss fn: 0.06775826215744019
iteration: 300444, loss fn: 0.06522443145513535
iteration: 300445, loss fn: 0.09087101370096207
iteration: 300446, loss fn: 0.055264685302972794
iteration: 300447, loss fn: 0.029160691425204277
iteration: 300448, loss fn: 0.06944967806339264
iteration: 300449, loss fn: 0.05625706538558006
iteration: 300450, loss fn: 0.09136233478784561
iteration: 300451, loss fn: 0.036242034286260605
iteration: 300452, loss fn: 0.037167083472013474
iteration: 300453, loss fn: 0.05036178603768349
iteration: 300454, loss fn: 0.08471663296222687
iteration: 300455, loss fn: 0.027257895097136497
iteration: 300456, loss fn: 0.04980812966823578
iteration: 300457, loss fn: 0.0644955262541771
iteration: 300458, loss fn: 0.034047652035951614
iteration: 300459, loss fn: 0.09467828273773193
iteration: 300460, loss fn: 0.05969943478703499
iteration: 300461, loss fn: 0.04862865060567856
iteration: 300462, loss fn: 0.10353974997997284
iteration: 300463, loss fn: 0.05984053015708923
iteration: 300464, loss fn: 0.045854561030864716
iteration: 300465, loss fn: 0.06906972825527191
iteration: 300466, loss fn: 0.07790647447109222
iteration: 300467, loss fn: 0.05371676757931709
iteration: 300468, loss fn: 0.07652789354324341
iteration: 300469, loss fn: 0.042790092527866364
iteration: 300470, loss fn: 0.059395160526037216
iteration: 300471, loss fn: 0.0644311010837555
iteration: 300472, loss fn: 0.06011439487338066
iteration: 300473, loss fn: 0.07742011547088623
iteration: 300474, loss fn: 0.08326119184494019
iteration: 300475, loss fn: 0.10829588025808334
iteration: 300476, loss fn: 0.04456291347742081
iteration: 300477, loss fn: 0.1163507029414177
iteration: 300478, loss fn: 0.04011796787381172
iteration: 300479, loss fn: 0.0403096079826355
iteration: 300480, loss fn: 0.036767225712537766
iteration: 300481, loss fn: 0.04138300567865372
iteration: 300482, loss fn: 0.053192138671875
iteration: 300483, loss fn: 0.05516364797949791
iteration: 300484, loss fn: 0.041252389550209045
iteration: 300485, loss fn: 0.03551655262708664
iteration: 300486, loss fn: 0.036292754113674164
iteration: 300487, loss fn: 0.049757350236177444
iteration: 300488, loss fn: 0.0995262861251831
iteration: 300489, loss fn: 0.08481290936470032
iteration: 300490, loss fn: 0.022295499220490456
iteration: 300491, loss fn: 0.05749152973294258
iteration: 300492, loss fn: 0.060751862823963165
iteration: 300493, loss fn: 0.05022488906979561
iteration: 300494, loss fn: 0.048290152102708817
iteration: 300495, loss fn: 0.041118767112493515
iteration: 300496, loss fn: 0.039652712643146515
iteration: 300497, loss fn: 0.040515266358852386
iteration: 300498, loss fn: 0.05148549750447273
iteration: 300499, loss fn: 0.03143669664859772
iteration: 300500, loss fn: 0.05354480817914009
iteration: 300501, loss fn: 0.03089403733611107
iteration: 300502, loss fn: 0.038094136863946915
iteration: 300503, loss fn: 0.03769378736615181
iteration: 300504, loss fn: 0.07323388755321503
iteration: 300505, loss fn: 0.05121954530477524
iteration: 300506, loss fn: 0.0855332687497139
iteration: 300507, loss fn: 0.0375998318195343
iteration: 300508, loss fn: 0.05456567928195
iteration: 300509, loss fn: 0.05456768721342087
iteration: 300510, loss fn: 0.03727509453892708
iteration: 300511, loss fn: 0.030448637902736664
iteration: 300512, loss fn: 0.04988168552517891
iteration: 300513, loss fn: 0.11253917962312698
iteration: 300514, loss fn: 0.09897609055042267
iteration: 300515, loss fn: 0.07474539428949356
iteration: 300516, loss fn: 0.06665797531604767
iteration: 300517, loss fn: 0.06201790273189545
iteration: 300518, loss fn: 0.04768611118197441
iteration: 300519, loss fn: 0.07126042991876602
iteration: 300520, loss fn: 0.0714278519153595
iteration: 300521, loss fn: 0.07282754778862
iteration: 300522, loss fn: 0.045157212764024734
iteration: 300523, loss fn: 0.07351034134626389
iteration: 300524, loss fn: 0.03701765462756157
iteration: 300525, loss fn: 0.07833269983530045
iteration: 300526, loss fn: 0.042042795568704605
iteration: 300527, loss fn: 0.08401359617710114
iteration: 300528, loss fn: 0.06909871846437454
iteration: 300529, loss fn: 0.09997211396694183
iteration: 300530, loss fn: 0.07735428214073181
iteration: 300531, loss fn: 0.051826175302267075
iteration: 300532, loss fn: 0.03659088537096977
iteration: 300533, loss fn: 0.046865638345479965
iteration: 300534, loss fn: 0.07393645495176315
iteration: 300535, loss fn: 0.02439584583044052
iteration: 300536, loss fn: 0.06313151121139526
iteration: 300537, loss fn: 0.04319112002849579
iteration: 300538, loss fn: 0.060614969581365585
iteration: 300539, loss fn: 0.06535298377275467
iteration: 300540, loss fn: 0.058712683618068695
iteration: 300541, loss fn: 0.030640745535492897
iteration: 300542, loss fn: 0.06281545758247375
iteration: 300543, loss fn: 0.036054827272892
iteration: 300544, loss fn: 0.022761022672057152
iteration: 300545, loss fn: 0.03023085556924343
iteration: 300546, loss fn: 0.042413752526044846
iteration: 300547, loss fn: 0.04705553501844406
iteration: 300548, loss fn: 0.06569182872772217
iteration: 300549, loss fn: 0.08733043819665909
iteration: 300550, loss fn: 0.07304466515779495
iteration: 300551, loss fn: 0.056303586810827255
iteration: 300552, loss fn: 0.05702740326523781
iteration: 300553, loss fn: 0.04567313194274902
iteration: 300554, loss fn: 0.057402320206165314
iteration: 300555, loss fn: 0.03674132376909256
iteration: 300556, loss fn: 0.10442425310611725
iteration: 300557, loss fn: 0.08983071893453598
iteration: 300558, loss fn: 0.03249863162636757
iteration: 300559, loss fn: 0.06899989396333694
iteration: 300560, loss fn: 0.054127614945173264
iteration: 300561, loss fn: 0.044121984392404556
iteration: 300562, loss fn: 0.0479634664952755
iteration: 300563, loss fn: 0.025376351550221443
iteration: 300564, loss fn: 0.06448835134506226
iteration: 300565, loss fn: 0.06785032898187637
iteration: 300566, loss fn: 0.05294869467616081
iteration: 300567, loss fn: 0.05063183605670929
iteration: 300568, loss fn: 0.07924452424049377
iteration: 300569, loss fn: 0.06293261796236038
iteration: 300570, loss fn: 0.052024733275175095
iteration: 300571, loss fn: 0.07412046939134598
iteration: 300572, loss fn: 0.055982768535614014
iteration: 300573, loss fn: 0.0824732705950737
iteration: 300574, loss fn: 0.03558411821722984
iteration: 300575, loss fn: 0.06809945404529572
iteration: 300576, loss fn: 0.04359997436404228
iteration: 300577, loss fn: 0.034129414707422256
iteration: 300578, loss fn: 0.07359723746776581
iteration: 300579, loss fn: 0.053012438118457794
iteration: 300580, loss fn: 0.08561325818300247
iteration: 300581, loss fn: 0.05447143688797951
iteration: 300582, loss fn: 0.06932424753904343
iteration: 300583, loss fn: 0.06153582036495209
iteration: 300584, loss fn: 0.03786148875951767
iteration: 300585, loss fn: 0.08238416910171509
iteration: 300586, loss fn: 0.05878451466560364
iteration: 300587, loss fn: 0.03615571931004524
iteration: 300588, loss fn: 0.0635591596364975
iteration: 300589, loss fn: 0.04970480129122734
iteration: 300590, loss fn: 0.09909573197364807
iteration: 300591, loss fn: 0.06412353366613388
iteration: 300592, loss fn: 0.08853098750114441
iteration: 300593, loss fn: 0.034804243594408035
iteration: 300594, loss fn: 0.09474309533834457
iteration: 300595, loss fn: 0.04545373469591141
iteration: 300596, loss fn: 0.06476029008626938
iteration: 300597, loss fn: 0.032560259103775024
iteration: 300598, loss fn: 0.05816112458705902
iteration: 300599, loss fn: 0.056596897542476654
iteration: 300600, loss fn: 0.03366607800126076
iteration: 300601, loss fn: 0.06487707048654556
iteration: 300602, loss fn: 0.07097567617893219
iteration: 300603, loss fn: 0.04337643086910248
iteration: 300604, loss fn: 0.05533544719219208
iteration: 300605, loss fn: 0.07718001306056976
iteration: 300606, loss fn: 0.044643335044384
iteration: 300607, loss fn: 0.03214403986930847
iteration: 300608, loss fn: 0.05282345414161682
iteration: 300609, loss fn: 0.0687081441283226
iteration: 300610, loss fn: 0.07276938110589981
iteration: 300611, loss fn: 0.07019513845443726
iteration: 300612, loss fn: 0.06667304039001465
iteration: 300613, loss fn: 0.04645803943276405
iteration: 300614, loss fn: 0.10407356172800064
iteration: 300615, loss fn: 0.06670744717121124
iteration: 300616, loss fn: 0.01652384363114834
iteration: 300617, loss fn: 0.052274882793426514
iteration: 300618, loss fn: 0.04994284361600876
iteration: 300619, loss fn: 0.047460902482271194
iteration: 300620, loss fn: 0.0512399822473526
iteration: 300621, loss fn: 0.034594859927892685
iteration: 300622, loss fn: 0.04868203401565552
iteration: 300623, loss fn: 0.06991247832775116
iteration: 300624, loss fn: 0.057631902396678925
iteration: 300625, loss fn: 0.07161208987236023
iteration: 300626, loss fn: 0.03166574612259865
iteration: 300627, loss fn: 0.05551144853234291
iteration: 300628, loss fn: 0.08841408044099808
iteration: 300629, loss fn: 0.07090530544519424
iteration: 300630, loss fn: 0.08963674306869507
iteration: 300631, loss fn: 0.05207165703177452
iteration: 300632, loss fn: 0.08035951852798462
iteration: 300633, loss fn: 0.021635346114635468
iteration: 300634, loss fn: 0.06137607991695404
iteration: 300635, loss fn: 0.06023356318473816
iteration: 300636, loss fn: 0.0597819983959198
iteration: 300637, loss fn: 0.09579261392354965
iteration: 300638, loss fn: 0.06487185508012772
iteration: 300639, loss fn: 0.04192985221743584
iteration: 300640, loss fn: 0.03081134893000126
iteration: 300641, loss fn: 0.046319276094436646
iteration: 300642, loss fn: 0.039290010929107666
iteration: 300643, loss fn: 0.031984854489564896
iteration: 300644, loss fn: 0.06012517213821411
iteration: 300645, loss fn: 0.024968711659312248
iteration: 300646, loss fn: 0.06851211190223694
iteration: 300647, loss fn: 0.07482941448688507
iteration: 300648, loss fn: 0.0501205250620842
iteration: 300649, loss fn: 0.0668395534157753
iteration: 300650, loss fn: 0.06244508922100067
iteration: 300651, loss fn: 0.05585953965783119
iteration: 300652, loss fn: 0.03908143937587738
iteration: 300653, loss fn: 0.05006138235330582
iteration: 300654, loss fn: 0.020298050716519356
iteration: 300655, loss fn: 0.06723674386739731
iteration: 300656, loss fn: 0.10966971516609192
iteration: 300657, loss fn: 0.0804930180311203
iteration: 300658, loss fn: 0.10233960300683975
iteration: 300659, loss fn: 0.05320004001259804
iteration: 300660, loss fn: 0.08627467602491379
iteration: 300661, loss fn: 0.10997731983661652
iteration: 300662, loss fn: 0.05979644134640694
iteration: 300663, loss fn: 0.08814705163240433
iteration: 300664, loss fn: 0.07446178793907166
iteration: 300665, loss fn: 0.0706252008676529
iteration: 300666, loss fn: 0.03377831354737282
iteration: 300667, loss fn: 0.018329624086618423
iteration: 300668, loss fn: 0.07709042727947235
iteration: 300669, loss fn: 0.04766083508729935
iteration: 300670, loss fn: 0.09743907302618027
iteration: 300671, loss fn: 0.035546429455280304
iteration: 300672, loss fn: 0.06252025067806244
iteration: 300673, loss fn: 0.05524129793047905
iteration: 300674, loss fn: 0.053502388298511505
iteration: 300675, loss fn: 0.0800437331199646
iteration: 300676, loss fn: 0.05887285992503166
iteration: 300677, loss fn: 0.09113612771034241
iteration: 300678, loss fn: 0.03455151617527008
iteration: 300679, loss fn: 0.04544118046760559
iteration: 300680, loss fn: 0.058625537902116776
iteration: 300681, loss fn: 0.0442359484732151
iteration: 300682, loss fn: 0.024080177769064903
iteration: 300683, loss fn: 0.05374395102262497
iteration: 300684, loss fn: 0.09176342189311981
iteration: 300685, loss fn: 0.037370529025793076
iteration: 300686, loss fn: 0.03098295070230961
iteration: 300687, loss fn: 0.058215878903865814
iteration: 300688, loss fn: 0.08015216141939163
iteration: 300689, loss fn: 0.05736500397324562
iteration: 300690, loss fn: 0.10876630246639252
iteration: 300691, loss fn: 0.12529338896274567
iteration: 300692, loss fn: 0.053080230951309204
iteration: 300693, loss fn: 0.0355856716632843
iteration: 300694, loss fn: 0.07678277790546417
iteration: 300695, loss fn: 0.06104106083512306
iteration: 300696, loss fn: 0.03955323249101639
iteration: 300697, loss fn: 0.07324939221143723
iteration: 300698, loss fn: 0.0709836408495903
iteration: 300699, loss fn: 0.072869211435318
iteration: 300700, loss fn: 0.04940244182944298
iteration: 300701, loss fn: 0.07094467431306839
iteration: 300702, loss fn: 0.036094095557928085
iteration: 300703, loss fn: 0.052563246339559555
iteration: 300704, loss fn: 0.06503992527723312
iteration: 300705, loss fn: 0.07347054779529572
iteration: 300706, loss fn: 0.11626522988080978
iteration: 300707, loss fn: 0.04125051945447922
iteration: 300708, loss fn: 0.1143028512597084
iteration: 300709, loss fn: 0.010321644134819508
iteration: 300710, loss fn: 0.07627972960472107
iteration: 300711, loss fn: 0.038655076175928116
iteration: 300712, loss fn: 0.09449667483568192
iteration: 300713, loss fn: 0.058295972645282745
iteration: 300714, loss fn: 0.08706343173980713
iteration: 300715, loss fn: 0.034878794103860855
iteration: 300716, loss fn: 0.06888384371995926
iteration: 300717, loss fn: 0.0532379075884819
iteration: 300718, loss fn: 0.03511367365717888
iteration: 300719, loss fn: 0.06414372473955154
iteration: 300720, loss fn: 0.08485563844442368
iteration: 300721, loss fn: 0.053872670978307724
iteration: 300722, loss fn: 0.06025121733546257
iteration: 300723, loss fn: 0.017662839964032173
iteration: 300724, loss fn: 0.07648372650146484
iteration: 300725, loss fn: 0.09447062760591507
iteration: 300726, loss fn: 0.04336709901690483
iteration: 300727, loss fn: 0.08169537782669067
iteration: 300728, loss fn: 0.04356184974312782
iteration: 300729, loss fn: 0.031218985095620155
iteration: 300730, loss fn: 0.10249105840921402
iteration: 300731, loss fn: 0.0537777915596962
iteration: 300732, loss fn: 0.05801680311560631
iteration: 300733, loss fn: 0.027950769290328026
iteration: 300734, loss fn: 0.03564943000674248
iteration: 300735, loss fn: 0.08188540488481522
iteration: 300736, loss fn: 0.0789855420589447
iteration: 300737, loss fn: 0.039946191012859344
iteration: 300738, loss fn: 0.06866616010665894
iteration: 300739, loss fn: 0.09212205559015274
iteration: 300740, loss fn: 0.04303000122308731
iteration: 300741, loss fn: 0.07916077226400375
iteration: 300742, loss fn: 0.10634925216436386
iteration: 300743, loss fn: 0.08620502054691315
iteration: 300744, loss fn: 0.08210979402065277
iteration: 300745, loss fn: 0.06877783685922623
iteration: 300746, loss fn: 0.06228009611368179
iteration: 300747, loss fn: 0.046882811933755875
iteration: 300748, loss fn: 0.058246128261089325
iteration: 300749, loss fn: 0.0696299597620964
iteration: 300750, loss fn: 0.055080920457839966
iteration: 300751, loss fn: 0.051734454929828644
iteration: 300752, loss fn: 0.024074237793684006
iteration: 300753, loss fn: 0.046517424285411835
iteration: 300754, loss fn: 0.07350524514913559
iteration: 300755, loss fn: 0.06372665613889694
iteration: 300756, loss fn: 0.037547625601291656
iteration: 300757, loss fn: 0.07672284543514252
iteration: 300758, loss fn: 0.0429658368229866
iteration: 300759, loss fn: 0.0679316520690918
iteration: 300760, loss fn: 0.043546341359615326
iteration: 300761, loss fn: 0.0398605614900589
iteration: 300762, loss fn: 0.059789322316646576
iteration: 300763, loss fn: 0.1008695662021637
iteration: 300764, loss fn: 0.04664137586951256
iteration: 300765, loss fn: 0.048627179116010666
iteration: 300766, loss fn: 0.04629040136933327
iteration: 300767, loss fn: 0.03380921110510826
iteration: 300768, loss fn: 0.08256034553050995
iteration: 300769, loss fn: 0.057729508727788925
iteration: 300770, loss fn: 0.0681992620229721
iteration: 300771, loss fn: 0.0736621543765068
iteration: 300772, loss fn: 0.049474239349365234
iteration: 300773, loss fn: 0.019147422164678574
iteration: 300774, loss fn: 0.0671020895242691
iteration: 300775, loss fn: 0.08363168686628342
iteration: 300776, loss fn: 0.09001118689775467
iteration: 300777, loss fn: 0.059568483382463455
iteration: 300778, loss fn: 0.08869656175374985
iteration: 300779, loss fn: 0.06666635721921921
iteration: 300780, loss fn: 0.05466059222817421
iteration: 300781, loss fn: 0.11612080782651901
iteration: 300782, loss fn: 0.050592053681612015
iteration: 300783, loss fn: 0.05851000174880028
iteration: 300784, loss fn: 0.06092420592904091
iteration: 300785, loss fn: 0.0401688814163208
iteration: 300786, loss fn: 0.03194648399949074
iteration: 300787, loss fn: 0.040835779160261154
iteration: 300788, loss fn: 0.05713237449526787
iteration: 300789, loss fn: 0.04192167893052101
iteration: 300790, loss fn: 0.03447742760181427
iteration: 300791, loss fn: 0.04580498859286308
iteration: 300792, loss fn: 0.05980899930000305
iteration: 300793, loss fn: 0.021490298211574554
iteration: 300794, loss fn: 0.04334381967782974
iteration: 300795, loss fn: 0.07963427156209946
iteration: 300796, loss fn: 0.022967269644141197
iteration: 300797, loss fn: 0.05173119530081749
iteration: 300798, loss fn: 0.0407683327794075
iteration: 300799, loss fn: 0.06487535685300827
iteration: 300800, loss fn: 0.04742901399731636
iteration: 300801, loss fn: 0.04488006979227066
iteration: 300802, loss fn: 0.035934168845415115
iteration: 300803, loss fn: 0.06714244186878204
iteration: 300804, loss fn: 0.04527335613965988
iteration: 300805, loss fn: 0.0734606385231018
iteration: 300806, loss fn: 0.04516996070742607
iteration: 300807, loss fn: 0.06684491783380508
iteration: 300808, loss fn: 0.06620943546295166
iteration: 300809, loss fn: 0.09331972897052765
iteration: 300810, loss fn: 0.05829552561044693
iteration: 300811, loss fn: 0.03480811417102814
iteration: 300812, loss fn: 0.07747417688369751
iteration: 300813, loss fn: 0.11339159309864044
iteration: 300814, loss fn: 0.04650679975748062
iteration: 300815, loss fn: 0.06038340926170349
iteration: 300816, loss fn: 0.05387524887919426
iteration: 300817, loss fn: 0.09207262098789215
iteration: 300818, loss fn: 0.09996639937162399
iteration: 300819, loss fn: 0.10805826634168625
iteration: 300820, loss fn: 0.09485653787851334
iteration: 300821, loss fn: 0.05678747594356537
iteration: 300822, loss fn: 0.08863024413585663
iteration: 300823, loss fn: 0.05597015097737312
iteration: 300824, loss fn: 0.08507468551397324
iteration: 300825, loss fn: 0.06291209161281586
iteration: 300826, loss fn: 0.016290493309497833
iteration: 300827, loss fn: 0.06738565117120743
iteration: 300828, loss fn: 0.07370349019765854
iteration: 300829, loss fn: 0.03312567248940468
iteration: 300830, loss fn: 0.058620139956474304
iteration: 300831, loss fn: 0.03406296670436859
iteration: 300832, loss fn: 0.030548907816410065
iteration: 300833, loss fn: 0.07589024305343628
iteration: 300834, loss fn: 0.11397991329431534
iteration: 300835, loss fn: 0.0981413871049881
iteration: 300836, loss fn: 0.07007946074008942
iteration: 300837, loss fn: 0.08169148862361908
iteration: 300838, loss fn: 0.08445952832698822
iteration: 300839, loss fn: 0.05960269272327423
iteration: 300840, loss fn: 0.08940791338682175
iteration: 300841, loss fn: 0.04358231648802757
iteration: 300842, loss fn: 0.10430541634559631
iteration: 300843, loss fn: 0.07593099772930145
iteration: 300844, loss fn: 0.09696865826845169
iteration: 300845, loss fn: 0.09062250703573227
iteration: 300846, loss fn: 0.032222144305706024
iteration: 300847, loss fn: 0.04929650202393532
iteration: 300848, loss fn: 0.051913514733314514
iteration: 300849, loss fn: 0.07258834689855576
iteration: 300850, loss fn: 0.049155618995428085
iteration: 300851, loss fn: 0.04532700404524803
iteration: 300852, loss fn: 0.04172520339488983
iteration: 300853, loss fn: 0.055896058678627014
iteration: 300854, loss fn: 0.08117407560348511
iteration: 300855, loss fn: 0.05459523946046829
iteration: 300856, loss fn: 0.03884236514568329
iteration: 300857, loss fn: 0.05025505647063255
iteration: 300858, loss fn: 0.06201229244470596
iteration: 300859, loss fn: 0.06697452813386917
iteration: 300860, loss fn: 0.09555595368146896
iteration: 300861, loss fn: 0.06495647877454758
iteration: 300862, loss fn: 0.02353447861969471
iteration: 300863, loss fn: 0.026898903772234917
iteration: 300864, loss fn: 0.04709349200129509
iteration: 300865, loss fn: 0.08817237615585327
iteration: 300866, loss fn: 0.039205681532621384
iteration: 300867, loss fn: 0.026074308902025223
iteration: 300868, loss fn: 0.08322438597679138
iteration: 300869, loss fn: 0.039740029722452164
iteration: 300870, loss fn: 0.035387083888053894
iteration: 300871, loss fn: 0.035466067492961884
iteration: 300872, loss fn: 0.03940058499574661
iteration: 300873, loss fn: 0.04901537671685219
iteration: 300874, loss fn: 0.08886577934026718
iteration: 300875, loss fn: 0.023577909916639328
iteration: 300876, loss fn: 0.07389765977859497
iteration: 300877, loss fn: 0.05655607953667641
iteration: 300878, loss fn: 0.05625462159514427
iteration: 300879, loss fn: 0.011720743961632252
iteration: 300880, loss fn: 0.05287933349609375
iteration: 300881, loss fn: 0.08146069198846817
iteration: 300882, loss fn: 0.06077396497130394
iteration: 300883, loss fn: 0.0407421700656414
iteration: 300884, loss fn: 0.04926668852567673
iteration: 300885, loss fn: 0.06469748914241791
iteration: 300886, loss fn: 0.018459703773260117
iteration: 300887, loss fn: 0.056404974311590195
iteration: 300888, loss fn: 0.03430711105465889
iteration: 300889, loss fn: 0.03036549687385559
iteration: 300890, loss fn: 0.045071445405483246
iteration: 300891, loss fn: 0.03628610074520111
iteration: 300892, loss fn: 0.034513700753450394
iteration: 300893, loss fn: 0.06868580728769302
iteration: 300894, loss fn: 0.023243878036737442
iteration: 300895, loss fn: 0.049131814390420914
iteration: 300896, loss fn: 0.06706124544143677
iteration: 300897, loss fn: 0.04028528556227684
iteration: 300898, loss fn: 0.057041436433792114
iteration: 300899, loss fn: 0.07312708348035812
iteration: 300900, loss fn: 0.08160178363323212
iteration: 300901, loss fn: 0.05850492790341377
iteration: 300902, loss fn: 0.05966819077730179
iteration: 300903, loss fn: 0.03168325126171112
iteration: 300904, loss fn: 0.05429701507091522
iteration: 300905, loss fn: 0.06736759841442108
iteration: 300906, loss fn: 0.05911829322576523
iteration: 300907, loss fn: 0.0634424239397049
iteration: 300908, loss fn: 0.06938543915748596
iteration: 300909, loss fn: 0.05276687815785408
iteration: 300910, loss fn: 0.08630508184432983
iteration: 300911, loss fn: 0.07430952787399292
iteration: 300912, loss fn: 0.06300073862075806
iteration: 300913, loss fn: 0.07725746929645538
iteration: 300914, loss fn: 0.09652691334486008
iteration: 300915, loss fn: 0.09358282387256622
iteration: 300916, loss fn: 0.048691216856241226
iteration: 300917, loss fn: 0.06953746825456619
iteration: 300918, loss fn: 0.0726027861237526
iteration: 300919, loss fn: 0.04095863550901413
iteration: 300920, loss fn: 0.04903231933712959
iteration: 300921, loss fn: 0.08139681816101074
iteration: 300922, loss fn: 0.02944665588438511
iteration: 300923, loss fn: 0.06196467578411102
iteration: 300924, loss fn: 0.07321754842996597
iteration: 300925, loss fn: 0.04907878860831261
iteration: 300926, loss fn: 0.04816427826881409
iteration: 300927, loss fn: 0.043314311653375626
iteration: 300928, loss fn: 0.12561430037021637
iteration: 300929, loss fn: 0.1022723913192749
iteration: 300930, loss fn: 0.03976886719465256
iteration: 300931, loss fn: 0.09142634272575378
iteration: 300932, loss fn: 0.05854804441332817
iteration: 300933, loss fn: 0.04999144375324249
iteration: 300934, loss fn: 0.0778006985783577
iteration: 300935, loss fn: 0.02881224825978279
iteration: 300936, loss fn: 0.07877183705568314
iteration: 300937, loss fn: 0.048338476568460464
iteration: 300938, loss fn: 0.04940522089600563
iteration: 300939, loss fn: 0.046414826065301895
iteration: 300940, loss fn: 0.07256744801998138
iteration: 300941, loss fn: 0.03492475673556328
iteration: 300942, loss fn: 0.06358082592487335
iteration: 300943, loss fn: 0.045063316822052
iteration: 300944, loss fn: 0.04284566640853882
iteration: 300945, loss fn: 0.04773879051208496
iteration: 300946, loss fn: 0.06383921205997467
iteration: 300947, loss fn: 0.023718930780887604
iteration: 300948, loss fn: 0.03830147162079811
iteration: 300949, loss fn: 0.056626781821250916
iteration: 300950, loss fn: 0.05306003615260124
iteration: 300951, loss fn: 0.08666734397411346
iteration: 300952, loss fn: 0.0795484408736229
iteration: 300953, loss fn: 0.07966982573270798
iteration: 300954, loss fn: 0.07048048824071884
iteration: 300955, loss fn: 0.03957229107618332
iteration: 300956, loss fn: 0.05625459551811218
iteration: 300957, loss fn: 0.07013177126646042
iteration: 300958, loss fn: 0.0589742548763752
iteration: 300959, loss fn: 0.035377137362957
iteration: 300960, loss fn: 0.06837566941976547
iteration: 300961, loss fn: 0.06317811459302902
iteration: 300962, loss fn: 0.05199866369366646
iteration: 300963, loss fn: 0.042593374848365784
iteration: 300964, loss fn: 0.06499189883470535
iteration: 300965, loss fn: 0.0777706503868103
iteration: 300966, loss fn: 0.09908891469240189
iteration: 300967, loss fn: 0.062166228890419006
iteration: 300968, loss fn: 0.05706223100423813
iteration: 300969, loss fn: 0.05828561261296272
iteration: 300970, loss fn: 0.08232839405536652
iteration: 300971, loss fn: 0.12807676196098328
iteration: 300972, loss fn: 0.051603153347969055
iteration: 300973, loss fn: 0.0937800407409668
iteration: 300974, loss fn: 0.057298459112644196
iteration: 300975, loss fn: 0.06384284794330597
iteration: 300976, loss fn: 0.06597385555505753
iteration: 300977, loss fn: 0.06529495120048523
iteration: 300978, loss fn: 0.08619599789381027
iteration: 300979, loss fn: 0.05581723526120186
iteration: 300980, loss fn: 0.06070870906114578
iteration: 300981, loss fn: 0.03222065046429634
iteration: 300982, loss fn: 0.06724508851766586
iteration: 300983, loss fn: 0.06193676218390465
iteration: 300984, loss fn: 0.052342385053634644
iteration: 300985, loss fn: 0.08762483298778534
iteration: 300986, loss fn: 0.07182266563177109
iteration: 300987, loss fn: 0.08544443547725677
iteration: 300988, loss fn: 0.03969105705618858
iteration: 300989, loss fn: 0.04586814343929291
iteration: 300990, loss fn: 0.02421780303120613
iteration: 300991, loss fn: 0.05651146546006203
iteration: 300992, loss fn: 0.0584239736199379
iteration: 300993, loss fn: 0.11681823432445526
iteration: 300994, loss fn: 0.04976179823279381
iteration: 300995, loss fn: 0.05356350168585777
iteration: 300996, loss fn: 0.042810115963220596
iteration: 300997, loss fn: 0.08592303842306137
iteration: 300998, loss fn: 0.05042514204978943
iteration: 300999, loss fn: 0.03546735644340515
iteration: 301000, loss fn: 0.0491861067712307
iteration: 301001, loss fn: 0.049910057336091995
iteration: 301002, loss fn: 0.06284736841917038
iteration: 301003, loss fn: 0.04103884473443031
iteration: 301004, loss fn: 0.0405166931450367
iteration: 301005, loss fn: 0.07203396409749985
iteration: 301006, loss fn: 0.07798435539007187
iteration: 301007, loss fn: 0.043621595948934555
iteration: 301008, loss fn: 0.04458693042397499
iteration: 301009, loss fn: 0.05785302072763443
iteration: 301010, loss fn: 0.10097822546958923
iteration: 301011, loss fn: 0.07310425490140915
iteration: 301012, loss fn: 0.07985584437847137
iteration: 301013, loss fn: 0.04529689997434616
iteration: 301014, loss fn: 0.05637602135539055
iteration: 301015, loss fn: 0.08167332410812378
iteration: 301016, loss fn: 0.02472708933055401
iteration: 301017, loss fn: 0.054393310099840164
iteration: 301018, loss fn: 0.03911465033888817
iteration: 301019, loss fn: 0.05241869017481804
iteration: 301020, loss fn: 0.041165173053741455
iteration: 301021, loss fn: 0.07930333912372589
iteration: 301022, loss fn: 0.06150254234671593
iteration: 301023, loss fn: 0.059612296521663666
iteration: 301024, loss fn: 0.05558839067816734
iteration: 301025, loss fn: 0.07297755032777786
iteration: 301026, loss fn: 0.05576511472463608
iteration: 301027, loss fn: 0.08126528561115265
iteration: 301028, loss fn: 0.043114855885505676
iteration: 301029, loss fn: 0.06372035294771194
iteration: 301030, loss fn: 0.09651664644479752
iteration: 301031, loss fn: 0.03531322255730629
iteration: 301032, loss fn: 0.07868719846010208
iteration: 301033, loss fn: 0.05964360386133194
iteration: 301034, loss fn: 0.0659259557723999
iteration: 301035, loss fn: 0.030301066115498543
iteration: 301036, loss fn: 0.0791732519865036
iteration: 301037, loss fn: 0.08255506306886673
iteration: 301038, loss fn: 0.08494683355093002
iteration: 301039, loss fn: 0.050515152513980865
iteration: 301040, loss fn: 0.060827575623989105
iteration: 301041, loss fn: 0.07167305797338486
iteration: 301042, loss fn: 0.04615535959601402
iteration: 301043, loss fn: 0.04962514340877533
iteration: 301044, loss fn: 0.05895523354411125
iteration: 301045, loss fn: 0.040947314351797104
iteration: 301046, loss fn: 0.03719296678900719
iteration: 301047, loss fn: 0.0665542408823967
iteration: 301048, loss fn: 0.03978049010038376
iteration: 301049, loss fn: 0.07301513105630875
iteration: 301050, loss fn: 0.09832917898893356
iteration: 301051, loss fn: 0.0463678315281868
iteration: 301052, loss fn: 0.048211853951215744
iteration: 301053, loss fn: 0.09384819865226746
iteration: 301054, loss fn: 0.051308948546648026
iteration: 301055, loss fn: 0.056307919323444366
iteration: 301056, loss fn: 0.06466638296842575
iteration: 301057, loss fn: 0.049624253064394
iteration: 301058, loss fn: 0.05875176563858986
iteration: 301059, loss fn: 0.07327825576066971
iteration: 301060, loss fn: 0.07484064251184464
iteration: 301061, loss fn: 0.046084191650152206
iteration: 301062, loss fn: 0.04620630294084549
iteration: 301063, loss fn: 0.04399409145116806
iteration: 301064, loss fn: 0.07039079070091248
iteration: 301065, loss fn: 0.09360366314649582
iteration: 301066, loss fn: 0.07581745833158493
iteration: 301067, loss fn: 0.08099807798862457
iteration: 301068, loss fn: 0.02654908411204815
iteration: 301069, loss fn: 0.041168212890625
iteration: 301070, loss fn: 0.0725393071770668
iteration: 301071, loss fn: 0.07414208352565765
iteration: 301072, loss fn: 0.09841189533472061
iteration: 301073, loss fn: 0.037925418466329575
iteration: 301074, loss fn: 0.04751910641789436
iteration: 301075, loss fn: 0.04040439426898956
iteration: 301076, loss fn: 0.08173643797636032
iteration: 301077, loss fn: 0.05561140924692154
iteration: 301078, loss fn: 0.03566933795809746
iteration: 301079, loss fn: 0.04966213181614876
iteration: 301080, loss fn: 0.036001529544591904
iteration: 301081, loss fn: 0.07507909834384918
iteration: 301082, loss fn: 0.04494580999016762
iteration: 301083, loss fn: 0.03877617046236992
iteration: 301084, loss fn: 0.05938108637928963
iteration: 301085, loss fn: 0.05151553452014923
iteration: 301086, loss fn: 0.03653234243392944
iteration: 301087, loss fn: 0.06876654922962189
iteration: 301088, loss fn: 0.058168843388557434
iteration: 301089, loss fn: 0.07282957434654236
iteration: 301090, loss fn: 0.05682995170354843
iteration: 301091, loss fn: 0.047125253826379776
iteration: 301092, loss fn: 0.06397245824337006
iteration: 301093, loss fn: 0.0642460510134697
iteration: 301094, loss fn: 0.054565925151109695
iteration: 301095, loss fn: 0.01725257746875286
iteration: 301096, loss fn: 0.13406461477279663
iteration: 301097, loss fn: 0.0631926879286766
iteration: 301098, loss fn: 0.09597424417734146
iteration: 301099, loss fn: 0.04824124276638031
iteration: 301100, loss fn: 0.08818987011909485
iteration: 301101, loss fn: 0.04127068072557449
iteration: 301102, loss fn: 0.06127026677131653
iteration: 301103, loss fn: 0.05237903445959091
iteration: 301104, loss fn: 0.04461872950196266
iteration: 301105, loss fn: 0.08809709548950195
iteration: 301106, loss fn: 0.06285320222377777
iteration: 301107, loss fn: 0.06051155552268028
iteration: 301108, loss fn: 0.09189383685588837
iteration: 301109, loss fn: 0.045498788356781006
iteration: 301110, loss fn: 0.06633789092302322
iteration: 301111, loss fn: 0.07465210556983948
iteration: 301112, loss fn: 0.057452842593193054
iteration: 301113, loss fn: 0.03903932124376297
iteration: 301114, loss fn: 0.09172853827476501
iteration: 301115, loss fn: 0.07515318691730499
iteration: 301116, loss fn: 0.06517982482910156
iteration: 301117, loss fn: 0.08274628967046738
iteration: 301118, loss fn: 0.03531273826956749
iteration: 301119, loss fn: 0.060661010444164276
iteration: 301120, loss fn: 0.06746775656938553
iteration: 301121, loss fn: 0.031802743673324585
iteration: 301122, loss fn: 0.055157165974378586
iteration: 301123, loss fn: 0.04375838860869408
iteration: 301124, loss fn: 0.0725969672203064
iteration: 301125, loss fn: 0.07697901129722595
iteration: 301126, loss fn: 0.05476118624210358
iteration: 301127, loss fn: 0.04172670096158981
iteration: 301128, loss fn: 0.04072568565607071
iteration: 301129, loss fn: 0.05951521173119545
iteration: 301130, loss fn: 0.07169366627931595
iteration: 301131, loss fn: 0.06722451001405716
iteration: 301132, loss fn: 0.057088129222393036
iteration: 301133, loss fn: 0.03978240489959717
iteration: 301134, loss fn: 0.04062274470925331
iteration: 301135, loss fn: 0.0603083036839962
iteration: 301136, loss fn: 0.06961341202259064
iteration: 301137, loss fn: 0.04737422987818718
iteration: 301138, loss fn: 0.0884016901254654
iteration: 301139, loss fn: 0.051994066685438156
iteration: 301140, loss fn: 0.08634994179010391
iteration: 301141, loss fn: 0.08126045763492584
iteration: 301142, loss fn: 0.056337010115385056
iteration: 301143, loss fn: 0.04506297409534454
iteration: 301144, loss fn: 0.03348178789019585
iteration: 301145, loss fn: 0.016454871743917465
iteration: 301146, loss fn: 0.047241587191820145
iteration: 301147, loss fn: 0.07330451905727386
iteration: 301148, loss fn: 0.052487678825855255
iteration: 301149, loss fn: 0.06414206326007843
iteration: 301150, loss fn: 0.05666792020201683
iteration: 301151, loss fn: 0.07116184383630753
iteration: 301152, loss fn: 0.06687095761299133
iteration: 301153, loss fn: 0.1052733063697815
iteration: 301154, loss fn: 0.05855568125844002
iteration: 301155, loss fn: 0.1082499697804451
iteration: 301156, loss fn: 0.0480746254324913
iteration: 301157, loss fn: 0.06588662415742874
iteration: 301158, loss fn: 0.07649269700050354
iteration: 301159, loss fn: 0.04611333832144737
iteration: 301160, loss fn: 0.07224643230438232
iteration: 301161, loss fn: 0.045670993626117706
iteration: 301162, loss fn: 0.044202931225299835
iteration: 301163, loss fn: 0.06788132339715958
iteration: 301164, loss fn: 0.07769417017698288
iteration: 301165, loss fn: 0.04746389761567116
iteration: 301166, loss fn: 0.10080801695585251
iteration: 301167, loss fn: 0.039683036506175995
iteration: 301168, loss fn: 0.06592490524053574
iteration: 301169, loss fn: 0.029729818925261497
iteration: 301170, loss fn: 0.10642009973526001
iteration: 301171, loss fn: 0.08503022789955139
iteration: 301172, loss fn: 0.07438754290342331
iteration: 301173, loss fn: 0.08242558687925339
iteration: 301174, loss fn: 0.033004678785800934
iteration: 301175, loss fn: 0.10385427623987198
iteration: 301176, loss fn: 0.019517403095960617
iteration: 301177, loss fn: 0.0444876104593277
iteration: 301178, loss fn: 0.05661723017692566
iteration: 301179, loss fn: 0.07515714317560196
iteration: 301180, loss fn: 0.08860546350479126
iteration: 301181, loss fn: 0.052882615476846695
iteration: 301182, loss fn: 0.021670814603567123
iteration: 301183, loss fn: 0.03709471970796585
iteration: 301184, loss fn: 0.033149126917123795
iteration: 301185, loss fn: 0.05995556712150574
iteration: 301186, loss fn: 0.08819176256656647
iteration: 301187, loss fn: 0.053249381482601166
iteration: 301188, loss fn: 0.08410096168518066
iteration: 301189, loss fn: 0.06234881654381752
iteration: 301190, loss fn: 0.04480323940515518
iteration: 301191, loss fn: 0.059232212603092194
iteration: 301192, loss fn: 0.08615053445100784
iteration: 301193, loss fn: 0.07690718024969101
iteration: 301194, loss fn: 0.05899704992771149
iteration: 301195, loss fn: 0.09730071574449539
iteration: 301196, loss fn: 0.06034369021654129
iteration: 301197, loss fn: 0.0835471898317337
iteration: 301198, loss fn: 0.07906603068113327
iteration: 301199, loss fn: 0.0544683113694191
iteration: 301200, loss fn: 0.0706925243139267
iteration: 301201, loss fn: 0.03803373500704765
iteration: 301202, loss fn: 0.05694252625107765
iteration: 301203, loss fn: 0.07455658912658691
iteration: 301204, loss fn: 0.04235198348760605
iteration: 301205, loss fn: 0.06228475645184517
iteration: 301206, loss fn: 0.1021161898970604
iteration: 301207, loss fn: 0.06645626574754715
iteration: 301208, loss fn: 0.10919418185949326
iteration: 301209, loss fn: 0.06436353176832199
iteration: 301210, loss fn: 0.06851483136415482
iteration: 301211, loss fn: 0.07604461163282394
iteration: 301212, loss fn: 0.04735139384865761
iteration: 301213, loss fn: 0.02787051349878311
iteration: 301214, loss fn: 0.06365439295768738
iteration: 301215, loss fn: 0.040068309754133224
iteration: 301216, loss fn: 0.0948282852768898
iteration: 301217, loss fn: 0.03741549327969551
iteration: 301218, loss fn: 0.07225865870714188
iteration: 301219, loss fn: 0.0780717134475708
iteration: 301220, loss fn: 0.03980810567736626
iteration: 301221, loss fn: 0.05906489118933678
iteration: 301222, loss fn: 0.04046935215592384
iteration: 301223, loss fn: 0.10693241655826569
iteration: 301224, loss fn: 0.06455966085195541
iteration: 301225, loss fn: 0.04673854634165764
iteration: 301226, loss fn: 0.02706952951848507
iteration: 301227, loss fn: 0.061396729201078415
iteration: 301228, loss fn: 0.07031191140413284
iteration: 301229, loss fn: 0.04495082423090935
iteration: 301230, loss fn: 0.06520942598581314
iteration: 301231, loss fn: 0.11164829134941101
iteration: 301232, loss fn: 0.034232426434755325
iteration: 301233, loss fn: 0.040729790925979614
iteration: 301234, loss fn: 0.07129459083080292
iteration: 301235, loss fn: 0.06595905125141144
iteration: 301236, loss fn: 0.024393415078520775
iteration: 301237, loss fn: 0.06676417589187622
iteration: 301238, loss fn: 0.013717460446059704
iteration: 301239, loss fn: 0.03911573067307472
iteration: 301240, loss fn: 0.06281644105911255
iteration: 301241, loss fn: 0.05258898809552193
iteration: 301242, loss fn: 0.08670571446418762
iteration: 301243, loss fn: 0.09274691343307495
iteration: 301244, loss fn: 0.08450903743505478
iteration: 301245, loss fn: 0.038001708686351776
iteration: 301246, loss fn: 0.06357337534427643
iteration: 301247, loss fn: 0.07694999128580093
iteration: 301248, loss fn: 0.09115855395793915
iteration: 301249, loss fn: 0.048910900950431824
iteration: 301250, loss fn: 0.0616687647998333
iteration: 301251, loss fn: 0.026718558743596077
iteration: 301252, loss fn: 0.025312310084700584
iteration: 301253, loss fn: 0.08264567703008652
iteration: 301254, loss fn: 0.09368181973695755
iteration: 301255, loss fn: 0.04801788553595543
iteration: 301256, loss fn: 0.08965340256690979
iteration: 301257, loss fn: 0.06272852420806885
iteration: 301258, loss fn: 0.05658116936683655
iteration: 301259, loss fn: 0.07490178197622299
iteration: 301260, loss fn: 0.034537188708782196
iteration: 301261, loss fn: 0.06971815228462219
iteration: 301262, loss fn: 0.03518792241811752
iteration: 301263, loss fn: 0.04172513261437416
iteration: 301264, loss fn: 0.06160494312644005
iteration: 301265, loss fn: 0.06779792904853821
iteration: 301266, loss fn: 0.01951749250292778
iteration: 301267, loss fn: 0.08163731545209885
iteration: 301268, loss fn: 0.07743354886770248
iteration: 301269, loss fn: 0.044658903032541275
iteration: 301270, loss fn: 0.04877195879817009
iteration: 301271, loss fn: 0.05870110169053078
iteration: 301272, loss fn: 0.06149706989526749
iteration: 301273, loss fn: 0.05511648580431938
iteration: 301274, loss fn: 0.06710302829742432
iteration: 301275, loss fn: 0.08574970066547394
iteration: 301276, loss fn: 0.04674720764160156
iteration: 301277, loss fn: 0.07166080921888351
iteration: 301278, loss fn: 0.03925514593720436
iteration: 301279, loss fn: 0.1154407411813736
iteration: 301280, loss fn: 0.02981715276837349
iteration: 301281, loss fn: 0.027656204998493195
iteration: 301282, loss fn: 0.054825559258461
iteration: 301283, loss fn: 0.07170206308364868
iteration: 301284, loss fn: 0.05324169248342514
iteration: 301285, loss fn: 0.05993996560573578
iteration: 301286, loss fn: 0.0377834290266037
iteration: 301287, loss fn: 0.14554619789123535
iteration: 301288, loss fn: 0.06114492565393448
iteration: 301289, loss fn: 0.08131984621286392
iteration: 301290, loss fn: 0.08602121472358704
iteration: 301291, loss fn: 0.08762127161026001
iteration: 301292, loss fn: 0.04118397831916809
iteration: 301293, loss fn: 0.04949416592717171
iteration: 301294, loss fn: 0.050675272941589355
iteration: 301295, loss fn: 0.033131904900074005
iteration: 301296, loss fn: 0.06847543269395828
iteration: 301297, loss fn: 0.07872205972671509
iteration: 301298, loss fn: 0.030158251523971558
iteration: 301299, loss fn: 0.06366883218288422
iteration: 301300, loss fn: 0.025787435472011566
iteration: 301301, loss fn: 0.04546317458152771
iteration: 301302, loss fn: 0.03537973389029503
iteration: 301303, loss fn: 0.07190899550914764
iteration: 301304, loss fn: 0.08928324282169342
iteration: 301305, loss fn: 0.0468294732272625
iteration: 301306, loss fn: 0.06286169588565826
iteration: 301307, loss fn: 0.05700236186385155
iteration: 301308, loss fn: 0.05681460723280907
iteration: 301309, loss fn: 0.07174119353294373
iteration: 301310, loss fn: 0.053681910037994385
iteration: 301311, loss fn: 0.06523215025663376
iteration: 301312, loss fn: 0.06759980320930481
iteration: 301313, loss fn: 0.11619923263788223
iteration: 301314, loss fn: 0.05506774038076401
iteration: 301315, loss fn: 0.01363564096391201
iteration: 301316, loss fn: 0.07751256227493286
iteration: 301317, loss fn: 0.06435154378414154
iteration: 301318, loss fn: 0.0513804629445076
iteration: 301319, loss fn: 0.08851802349090576
iteration: 301320, loss fn: 0.06921957433223724
iteration: 301321, loss fn: 0.08515819162130356
iteration: 301322, loss fn: 0.06279219686985016
iteration: 301323, loss fn: 0.11184313148260117
iteration: 301324, loss fn: 0.05492214486002922
iteration: 301325, loss fn: 0.053739868104457855
iteration: 301326, loss fn: 0.03834201768040657
iteration: 301327, loss fn: 0.04291745275259018
iteration: 301328, loss fn: 0.04514281824231148
iteration: 301329, loss fn: 0.057923827320337296
iteration: 301330, loss fn: 0.05816284567117691
iteration: 301331, loss fn: 0.06050196290016174
iteration: 301332, loss fn: 0.07589013874530792
iteration: 301333, loss fn: 0.05506288260221481
iteration: 301334, loss fn: 0.04765871912240982
iteration: 301335, loss fn: 0.08972450345754623
iteration: 301336, loss fn: 0.060366880148649216
iteration: 301337, loss fn: 0.03898204118013382
iteration: 301338, loss fn: 0.04933447390794754
iteration: 301339, loss fn: 0.02623482421040535
iteration: 301340, loss fn: 0.04550692066550255
iteration: 301341, loss fn: 0.06104749068617821
iteration: 301342, loss fn: 0.07308091968297958
iteration: 301343, loss fn: 0.07026249915361404
iteration: 301344, loss fn: 0.038397274911403656
iteration: 301345, loss fn: 0.036580704152584076
iteration: 301346, loss fn: 0.04549311101436615
iteration: 301347, loss fn: 0.06500983983278275
iteration: 301348, loss fn: 0.04543087258934975
iteration: 301349, loss fn: 0.03179508075118065
iteration: 301350, loss fn: 0.0671316459774971
iteration: 301351, loss fn: 0.04400506988167763
iteration: 301352, loss fn: 0.048775844275951385
iteration: 301353, loss fn: 0.0953197255730629
iteration: 301354, loss fn: 0.10933457314968109
iteration: 301355, loss fn: 0.06376462429761887
iteration: 301356, loss fn: 0.06143014505505562
iteration: 301357, loss fn: 0.05022736266255379
iteration: 301358, loss fn: 0.06131287291646004
iteration: 301359, loss fn: 0.07438573986291885
iteration: 301360, loss fn: 0.10456164926290512
iteration: 301361, loss fn: 0.08121868222951889
iteration: 301362, loss fn: 0.08489689975976944
iteration: 301363, loss fn: 0.07657337933778763
iteration: 301364, loss fn: 0.1100105345249176
iteration: 301365, loss fn: 0.055620405822992325
iteration: 301366, loss fn: 0.06805456429719925
iteration: 301367, loss fn: 0.062425706535577774
iteration: 301368, loss fn: 0.08078537881374359
iteration: 301369, loss fn: 0.13432537019252777
iteration: 301370, loss fn: 0.11164086312055588
iteration: 301371, loss fn: 0.07313775271177292
iteration: 301372, loss fn: 0.06467809528112411
iteration: 301373, loss fn: 0.057016998529434204
iteration: 301374, loss fn: 0.14734581112861633
iteration: 301375, loss fn: 0.042664751410484314
iteration: 301376, loss fn: 0.04328763857483864
iteration: 301377, loss fn: 0.05304517224431038
iteration: 301378, loss fn: 0.07088906317949295
iteration: 301379, loss fn: 0.019616607576608658
iteration: 301380, loss fn: 0.06199057027697563
iteration: 301381, loss fn: 0.05401180312037468
iteration: 301382, loss fn: 0.04290119186043739
iteration: 301383, loss fn: 0.04714033007621765
iteration: 301384, loss fn: 0.04922572523355484
iteration: 301385, loss fn: 0.05663067102432251
iteration: 301386, loss fn: 0.06356058269739151
iteration: 301387, loss fn: 0.06810087710618973
iteration: 301388, loss fn: 0.08382215350866318
iteration: 301389, loss fn: 0.07654384523630142
iteration: 301390, loss fn: 0.07411985844373703
iteration: 301391, loss fn: 0.04937563091516495
iteration: 301392, loss fn: 0.08650035411119461
iteration: 301393, loss fn: 0.08954720199108124
iteration: 301394, loss fn: 0.0674041137099266
iteration: 301395, loss fn: 0.05968888849020004
iteration: 301396, loss fn: 0.07144607603549957
iteration: 301397, loss fn: 0.03754834458231926
iteration: 301398, loss fn: 0.05332934111356735
iteration: 301399, loss fn: 0.03751764073967934
iteration: 301400, loss fn: 0.09939905256032944
iteration: 301401, loss fn: 0.07939673960208893
iteration: 301402, loss fn: 0.038624994456768036
iteration: 301403, loss fn: 0.057717468589544296
iteration: 301404, loss fn: 0.04576833173632622
iteration: 301405, loss fn: 0.041223473846912384
iteration: 301406, loss fn: 0.056008387356996536
iteration: 301407, loss fn: 0.04278162121772766
iteration: 301408, loss fn: 0.06802920252084732
iteration: 301409, loss fn: 0.04676838219165802
iteration: 301410, loss fn: 0.050139978528022766
iteration: 301411, loss fn: 0.11806178838014603
iteration: 301412, loss fn: 0.05815247818827629
iteration: 301413, loss fn: 0.07473073154687881
iteration: 301414, loss fn: 0.027062812820076942
iteration: 301415, loss fn: 0.056455038487911224
iteration: 301416, loss fn: 0.04219110682606697
iteration: 301417, loss fn: 0.03029005415737629
iteration: 301418, loss fn: 0.05140285938978195
iteration: 301419, loss fn: 0.08368473500013351
iteration: 301420, loss fn: 0.02931145206093788
iteration: 301421, loss fn: 0.043366048485040665
iteration: 301422, loss fn: 0.07623659074306488
iteration: 301423, loss fn: 0.07999321818351746
iteration: 301424, loss fn: 0.08227939903736115
iteration: 301425, loss fn: 0.03760255128145218
iteration: 301426, loss fn: 0.04672257602214813
iteration: 301427, loss fn: 0.032636020332574844
iteration: 301428, loss fn: 0.024396860972046852
iteration: 301429, loss fn: 0.048362355679273605
iteration: 301430, loss fn: 0.0855213850736618
iteration: 301431, loss fn: 0.09709828346967697
iteration: 301432, loss fn: 0.09369110316038132
iteration: 301433, loss fn: 0.06574995070695877
iteration: 301434, loss fn: 0.033769525587558746
iteration: 301435, loss fn: 0.048711299896240234
iteration: 301436, loss fn: 0.07487989962100983
iteration: 301437, loss fn: 0.07081268727779388
iteration: 301438, loss fn: 0.12266494333744049
iteration: 301439, loss fn: 0.02583974413573742
iteration: 301440, loss fn: 0.09050293266773224
iteration: 301441, loss fn: 0.08226737380027771
iteration: 301442, loss fn: 0.08850113302469254
iteration: 301443, loss fn: 0.05596984922885895
iteration: 301444, loss fn: 0.055407583713531494
iteration: 301445, loss fn: 0.020412961021065712
iteration: 301446, loss fn: 0.05300944298505783
iteration: 301447, loss fn: 0.014978799037635326
iteration: 301448, loss fn: 0.07188641279935837
iteration: 301449, loss fn: 0.08040058612823486
iteration: 301450, loss fn: 0.05874102935194969
iteration: 301451, loss fn: 0.04586226865649223
iteration: 301452, loss fn: 0.08440353721380234
iteration: 301453, loss fn: 0.08650422841310501
iteration: 301454, loss fn: 0.05172285437583923
iteration: 301455, loss fn: 0.05303940549492836
iteration: 301456, loss fn: 0.0611819364130497
iteration: 301457, loss fn: 0.1098659560084343
iteration: 301458, loss fn: 0.09134892374277115
iteration: 301459, loss fn: 0.036080252379179
iteration: 301460, loss fn: 0.05234057083725929
iteration: 301461, loss fn: 0.10238233953714371
iteration: 301462, loss fn: 0.04057520255446434
iteration: 301463, loss fn: 0.06427227705717087
iteration: 301464, loss fn: 0.05267474800348282
iteration: 301465, loss fn: 0.08986420929431915
iteration: 301466, loss fn: 0.05999069660902023
iteration: 301467, loss fn: 0.04254942387342453
iteration: 301468, loss fn: 0.04661070182919502
iteration: 301469, loss fn: 0.0577092319726944
iteration: 301470, loss fn: 0.09091968834400177
iteration: 301471, loss fn: 0.10162970423698425
iteration: 301472, loss fn: 0.05161276087164879
iteration: 301473, loss fn: 0.10269524157047272
iteration: 301474, loss fn: 0.08462302386760712
iteration: 301475, loss fn: 0.04361709579825401
iteration: 301476, loss fn: 0.04914340376853943
iteration: 301477, loss fn: 0.06786463409662247
iteration: 301478, loss fn: 0.05682920664548874
iteration: 301479, loss fn: 0.08350252360105515
iteration: 301480, loss fn: 0.07386870682239532
iteration: 301481, loss fn: 0.07892849296331406
iteration: 301482, loss fn: 0.04875631257891655
iteration: 301483, loss fn: 0.059263624250888824
iteration: 301484, loss fn: 0.04341459646821022
iteration: 301485, loss fn: 0.06304486840963364
iteration: 301486, loss fn: 0.0590127669274807
iteration: 301487, loss fn: 0.04725611209869385
iteration: 301488, loss fn: 0.046059515327215195
iteration: 301489, loss fn: 0.0764729380607605
iteration: 301490, loss fn: 0.06857132166624069
iteration: 301491, loss fn: 0.04852361977100372
iteration: 301492, loss fn: 0.025902356952428818
iteration: 301493, loss fn: 0.06505289673805237
iteration: 301494, loss fn: 0.057160425931215286
iteration: 301495, loss fn: 0.06952981650829315
iteration: 301496, loss fn: 0.06482333689928055
iteration: 301497, loss fn: 0.09870307147502899
iteration: 301498, loss fn: 0.049974456429481506
iteration: 301499, loss fn: 0.04799814894795418
iteration: 301500, loss fn: 0.07648367434740067
iteration: 301501, loss fn: 0.04982063174247742
iteration: 301502, loss fn: 0.05454074591398239
iteration: 301503, loss fn: 0.07541009038686752
iteration: 301504, loss fn: 0.04737791419029236
iteration: 301505, loss fn: 0.056481581181287766
iteration: 301506, loss fn: 0.07731485366821289
iteration: 301507, loss fn: 0.03983021900057793
iteration: 301508, loss fn: 0.06126493215560913
iteration: 301509, loss fn: 0.0749695748090744
iteration: 301510, loss fn: 0.0390360988676548
iteration: 301511, loss fn: 0.039517439901828766
iteration: 301512, loss fn: 0.06108523905277252
iteration: 301513, loss fn: 0.14343023300170898
iteration: 301514, loss fn: 0.02475794218480587
iteration: 301515, loss fn: 0.07815958559513092
iteration: 301516, loss fn: 0.08253705501556396
iteration: 301517, loss fn: 0.09513982385396957
iteration: 301518, loss fn: 0.04898923635482788
iteration: 301519, loss fn: 0.026546118780970573
iteration: 301520, loss fn: 0.08995756506919861
iteration: 301521, loss fn: 0.12001063674688339
iteration: 301522, loss fn: 0.034545041620731354
iteration: 301523, loss fn: 0.06933917105197906
iteration: 301524, loss fn: 0.06421169638633728
iteration: 301525, loss fn: 0.08301740139722824
iteration: 301526, loss fn: 0.07141479849815369
iteration: 301527, loss fn: 0.042197875678539276
iteration: 301528, loss fn: 0.062155891209840775
iteration: 301529, loss fn: 0.07490601390600204
iteration: 301530, loss fn: 0.09171269088983536
iteration: 301531, loss fn: 0.07318949699401855
iteration: 301532, loss fn: 0.044261183589696884
iteration: 301533, loss fn: 0.038109902292490005
iteration: 301534, loss fn: 0.06026782467961311
iteration: 301535, loss fn: 0.05640507861971855
iteration: 301536, loss fn: 0.06410757452249527
iteration: 301537, loss fn: 0.04175271466374397
iteration: 301538, loss fn: 0.04617835581302643
iteration: 301539, loss fn: 0.05697031691670418
iteration: 301540, loss fn: 0.07049895077943802
iteration: 301541, loss fn: 0.03405703976750374
iteration: 301542, loss fn: 0.07480663806200027
iteration: 301543, loss fn: 0.05123535916209221
iteration: 301544, loss fn: 0.0882968008518219
iteration: 301545, loss fn: 0.07513054460287094
iteration: 301546, loss fn: 0.03880871832370758
iteration: 301547, loss fn: 0.09549418091773987
iteration: 301548, loss fn: 0.06164425238966942
iteration: 301549, loss fn: 0.054976191371679306
iteration: 301550, loss fn: 0.07101330906152725
iteration: 301551, loss fn: 0.06580175459384918
iteration: 301552, loss fn: 0.04170426353812218
iteration: 301553, loss fn: 0.049837626516819
iteration: 301554, loss fn: 0.06733868271112442
iteration: 301555, loss fn: 0.04034034535288811
iteration: 301556, loss fn: 0.04685412719845772
iteration: 301557, loss fn: 0.061112746596336365
iteration: 301558, loss fn: 0.05604294687509537
iteration: 301559, loss fn: 0.03840991482138634
iteration: 301560, loss fn: 0.07668896019458771
iteration: 301561, loss fn: 0.04367619752883911
iteration: 301562, loss fn: 0.07353299111127853
iteration: 301563, loss fn: 0.06753528118133545
iteration: 301564, loss fn: 0.07889164984226227
iteration: 301565, loss fn: 0.04543410614132881
iteration: 301566, loss fn: 0.025058168917894363
iteration: 301567, loss fn: 0.042081840336322784
iteration: 301568, loss fn: 0.07841802388429642
iteration: 301569, loss fn: 0.0597790852189064
iteration: 301570, loss fn: 0.07481106370687485
iteration: 301571, loss fn: 0.06367484480142593
iteration: 301572, loss fn: 0.06557071954011917
iteration: 301573, loss fn: 0.09169092774391174
iteration: 301574, loss fn: 0.0946275144815445
iteration: 301575, loss fn: 0.08048530668020248
iteration: 301576, loss fn: 0.06372631341218948
iteration: 301577, loss fn: 0.058739591389894485
iteration: 301578, loss fn: 0.04762371629476547
iteration: 301579, loss fn: 0.033354081213474274
iteration: 301580, loss fn: 0.09072692692279816
iteration: 301581, loss fn: 0.061404917389154434
iteration: 301582, loss fn: 0.063958540558815
iteration: 301583, loss fn: 0.03831183537840843
iteration: 301584, loss fn: 0.06273077428340912
iteration: 301585, loss fn: 0.04926076903939247
iteration: 301586, loss fn: 0.09958541393280029
iteration: 301587, loss fn: 0.055146723985672
iteration: 301588, loss fn: 0.050494588911533356
iteration: 301589, loss fn: 0.045447252690792084
iteration: 301590, loss fn: 0.05391527712345123
iteration: 301591, loss fn: 0.05025814473628998
iteration: 301592, loss fn: 0.07732824981212616
iteration: 301593, loss fn: 0.039604026824235916
iteration: 301594, loss fn: 0.13678854703903198
iteration: 301595, loss fn: 0.06475667655467987
iteration: 301596, loss fn: 0.03388447314500809
iteration: 301597, loss fn: 0.06582459062337875
iteration: 301598, loss fn: 0.030684910714626312
iteration: 301599, loss fn: 0.10802984982728958
iteration: 301600, loss fn: 0.10782035440206528
iteration: 301601, loss fn: 0.09302186965942383
iteration: 301602, loss fn: 0.07193432003259659
iteration: 301603, loss fn: 0.036158688366413116
iteration: 301604, loss fn: 0.0700431540608406
iteration: 301605, loss fn: 0.12127494066953659
iteration: 301606, loss fn: 0.07689659297466278
iteration: 301607, loss fn: 0.07227470725774765
iteration: 301608, loss fn: 0.06734851747751236
iteration: 301609, loss fn: 0.07333213090896606
iteration: 301610, loss fn: 0.061791565269231796
iteration: 301611, loss fn: 0.026969529688358307
iteration: 301612, loss fn: 0.0540134534239769
iteration: 301613, loss fn: 0.0863458514213562
iteration: 301614, loss fn: 0.06120732054114342
iteration: 301615, loss fn: 0.06860502809286118
iteration: 301616, loss fn: 0.07014619559049606
iteration: 301617, loss fn: 0.065766341984272
iteration: 301618, loss fn: 0.0641234815120697
iteration: 301619, loss fn: 0.04677229002118111
iteration: 301620, loss fn: 0.03762262687087059
iteration: 301621, loss fn: 0.06446832418441772
iteration: 301622, loss fn: 0.10606738924980164
iteration: 301623, loss fn: 0.056201525032520294
iteration: 301624, loss fn: 0.08495951443910599
iteration: 301625, loss fn: 0.0961790606379509
iteration: 301626, loss fn: 0.060193851590156555
iteration: 301627, loss fn: 0.06035971641540527
iteration: 301628, loss fn: 0.07458868622779846
iteration: 301629, loss fn: 0.03833742439746857
iteration: 301630, loss fn: 0.030413275584578514
iteration: 301631, loss fn: 0.0431005135178566
iteration: 301632, loss fn: 0.05695724859833717
iteration: 301633, loss fn: 0.051654670387506485
iteration: 301634, loss fn: 0.06898169964551926
iteration: 301635, loss fn: 0.040298283100128174
iteration: 301636, loss fn: 0.030981289222836494
iteration: 301637, loss fn: 0.10601309686899185
iteration: 301638, loss fn: 0.04018544778227806
iteration: 301639, loss fn: 0.06394481658935547
iteration: 301640, loss fn: 0.038106564432382584
iteration: 301641, loss fn: 0.048695363104343414
iteration: 301642, loss fn: 0.05826793983578682
iteration: 301643, loss fn: 0.042165447026491165
iteration: 301644, loss fn: 0.04750896245241165
iteration: 301645, loss fn: 0.06985470652580261
iteration: 301646, loss fn: 0.07443781197071075
iteration: 301647, loss fn: 0.017177361994981766
iteration: 301648, loss fn: 0.043814193457365036
iteration: 301649, loss fn: 0.037253689020872116
iteration: 301650, loss fn: 0.0192634928971529
iteration: 301651, loss fn: 0.10030720382928848
iteration: 301652, loss fn: 0.05024445801973343
iteration: 301653, loss fn: 0.04297488182783127
iteration: 301654, loss fn: 0.08156836777925491
iteration: 301655, loss fn: 0.0488136000931263
iteration: 301656, loss fn: 0.06733463704586029
iteration: 301657, loss fn: 0.03396778926253319
iteration: 301658, loss fn: 0.060988061130046844
iteration: 301659, loss fn: 0.04049328714609146
iteration: 301660, loss fn: 0.02292850986123085
iteration: 301661, loss fn: 0.0380886010825634
iteration: 301662, loss fn: 0.05180055648088455
iteration: 301663, loss fn: 0.048786915838718414
iteration: 301664, loss fn: 0.06663434952497482
iteration: 301665, loss fn: 0.04929395392537117
iteration: 301666, loss fn: 0.041474804282188416
iteration: 301667, loss fn: 0.06475651264190674
iteration: 301668, loss fn: 0.04778405651450157
iteration: 301669, loss fn: 0.07423298060894012
iteration: 301670, loss fn: 0.08229856193065643
iteration: 301671, loss fn: 0.0549357570707798
iteration: 301672, loss fn: 0.09972891211509705
iteration: 301673, loss fn: 0.06267768889665604
iteration: 301674, loss fn: 0.04043429717421532
iteration: 301675, loss fn: 0.045203499495983124
iteration: 301676, loss fn: 0.042822085320949554
iteration: 301677, loss fn: 0.054268013685941696
iteration: 301678, loss fn: 0.05249437317252159
iteration: 301679, loss fn: 0.038920316845178604
iteration: 301680, loss fn: 0.058136191219091415
iteration: 301681, loss fn: 0.07724673300981522
iteration: 301682, loss fn: 0.08455388247966766
iteration: 301683, loss fn: 0.05454922839999199
iteration: 301684, loss fn: 0.051245320588350296
iteration: 301685, loss fn: 0.055494751781225204
iteration: 301686, loss fn: 0.05153613165020943
iteration: 301687, loss fn: 0.06123622879385948
iteration: 301688, loss fn: 0.06075862795114517
iteration: 301689, loss fn: 0.06694452464580536
iteration: 301690, loss fn: 0.08112910389900208
iteration: 301691, loss fn: 0.03999674692749977
iteration: 301692, loss fn: 0.0711076632142067
iteration: 301693, loss fn: 0.05734473839402199
iteration: 301694, loss fn: 0.07124429941177368
iteration: 301695, loss fn: 0.06806573271751404
iteration: 301696, loss fn: 0.08131719380617142
iteration: 301697, loss fn: 0.043958328664302826
iteration: 301698, loss fn: 0.051316410303115845
iteration: 301699, loss fn: 0.08205205202102661
iteration: 301700, loss fn: 0.09938205033540726
iteration: 301701, loss fn: 0.053264468908309937
iteration: 301702, loss fn: 0.07998295873403549
iteration: 301703, loss fn: 0.03919696807861328
iteration: 301704, loss fn: 0.03718693181872368
iteration: 301705, loss fn: 0.0841277465224266
iteration: 301706, loss fn: 0.05809558182954788
iteration: 301707, loss fn: 0.036865539848804474
iteration: 301708, loss fn: 0.02423837035894394
iteration: 301709, loss fn: 0.09289868175983429
iteration: 301710, loss fn: 0.039012521505355835
iteration: 301711, loss fn: 0.08374647796154022
iteration: 301712, loss fn: 0.0644189864397049
iteration: 301713, loss fn: 0.07406444847583771
iteration: 301714, loss fn: 0.06464636325836182
iteration: 301715, loss fn: 0.06803335249423981
iteration: 301716, loss fn: 0.08018486201763153
iteration: 301717, loss fn: 0.0872444212436676
iteration: 301718, loss fn: 0.025852028280496597
iteration: 301719, loss fn: 0.039976365864276886
iteration: 301720, loss fn: 0.07910054922103882
iteration: 301721, loss fn: 0.028101621195673943
iteration: 301722, loss fn: 0.06532857567071915
iteration: 301723, loss fn: 0.04674094170331955
iteration: 301724, loss fn: 0.06410188972949982
iteration: 301725, loss fn: 0.09282674640417099
iteration: 301726, loss fn: 0.05758121609687805
iteration: 301727, loss fn: 0.06549753248691559
iteration: 301728, loss fn: 0.05143790692090988
iteration: 301729, loss fn: 0.07080230116844177
iteration: 301730, loss fn: 0.04236159473657608
iteration: 301731, loss fn: 0.03516997769474983
iteration: 301732, loss fn: 0.06282763183116913
iteration: 301733, loss fn: 0.021791823208332062
iteration: 301734, loss fn: 0.07000275701284409
iteration: 301735, loss fn: 0.06516200304031372
iteration: 301736, loss fn: 0.0659986287355423
iteration: 301737, loss fn: 0.04443885013461113
iteration: 301738, loss fn: 0.07473807781934738
iteration: 301739, loss fn: 0.06569705903530121
iteration: 301740, loss fn: 0.04244882985949516
iteration: 301741, loss fn: 0.03680753335356712
iteration: 301742, loss fn: 0.04834413155913353
iteration: 301743, loss fn: 0.09028089791536331
iteration: 301744, loss fn: 0.04592768847942352
iteration: 301745, loss fn: 0.07930809259414673
iteration: 301746, loss fn: 0.04834667593240738
iteration: 301747, loss fn: 0.04100329056382179
iteration: 301748, loss fn: 0.0635165199637413
iteration: 301749, loss fn: 0.07671327888965607
iteration: 301750, loss fn: 0.0699019804596901
iteration: 301751, loss fn: 0.078870989382267
iteration: 301752, loss fn: 0.03290986269712448
iteration: 301753, loss fn: 0.041489023715257645
iteration: 301754, loss fn: 0.047589920461177826
iteration: 301755, loss fn: 0.036287009716033936
iteration: 301756, loss fn: 0.06120854243636131
iteration: 301757, loss fn: 0.0909818783402443
iteration: 301758, loss fn: 0.04767078161239624
iteration: 301759, loss fn: 0.027505381032824516
iteration: 301760, loss fn: 0.01155110914260149
iteration: 301761, loss fn: 0.08183921128511429
iteration: 301762, loss fn: 0.05500924959778786
iteration: 301763, loss fn: 0.04840749129652977
iteration: 301764, loss fn: 0.06243753060698509
iteration: 301765, loss fn: 0.03377978131175041
iteration: 301766, loss fn: 0.03367934376001358
iteration: 301767, loss fn: 0.11819012463092804
iteration: 301768, loss fn: 0.048070333898067474
iteration: 301769, loss fn: 0.06226705387234688
iteration: 301770, loss fn: 0.1088719367980957
iteration: 301771, loss fn: 0.06893528997898102
iteration: 301772, loss fn: 0.048201970756053925
iteration: 301773, loss fn: 0.0522131472826004
iteration: 301774, loss fn: 0.04882564768195152
iteration: 301775, loss fn: 0.07667484879493713
iteration: 301776, loss fn: 0.08687617629766464
iteration: 301777, loss fn: 0.06747613847255707
iteration: 301778, loss fn: 0.0918346643447876
iteration: 301779, loss fn: 0.05854332074522972
iteration: 301780, loss fn: 0.054886940866708755
iteration: 301781, loss fn: 0.08925007283687592
iteration: 301782, loss fn: 0.06776326894760132
iteration: 301783, loss fn: 0.03180388733744621
iteration: 301784, loss fn: 0.05891493707895279
iteration: 301785, loss fn: 0.06411182135343552
iteration: 301786, loss fn: 0.04053078964352608
iteration: 301787, loss fn: 0.04558029770851135
iteration: 301788, loss fn: 0.04896634444594383
iteration: 301789, loss fn: 0.0485958606004715
iteration: 301790, loss fn: 0.05087615177035332
iteration: 301791, loss fn: 0.04843737930059433
iteration: 301792, loss fn: 0.0489518865942955
iteration: 301793, loss fn: 0.05151056498289108
iteration: 301794, loss fn: 0.07569189369678497
iteration: 301795, loss fn: 0.06969083845615387
iteration: 301796, loss fn: 0.057688429951667786
iteration: 301797, loss fn: 0.06039290875196457
iteration: 301798, loss fn: 0.0671306774020195
iteration: 301799, loss fn: 0.08080463856458664
iteration: 301800, loss fn: 0.07179183512926102
iteration: 301801, loss fn: 0.04031321033835411
iteration: 301802, loss fn: 0.04473147913813591
iteration: 301803, loss fn: 0.056299980729818344
iteration: 301804, loss fn: 0.07898068428039551
iteration: 301805, loss fn: 0.05596763640642166
iteration: 301806, loss fn: 0.04403296113014221
iteration: 301807, loss fn: 0.06335105001926422
iteration: 301808, loss fn: 0.055559270083904266
iteration: 301809, loss fn: 0.08548355847597122
iteration: 301810, loss fn: 0.07388296723365784
iteration: 301811, loss fn: 0.06397692114114761
iteration: 301812, loss fn: 0.07406537979841232
iteration: 301813, loss fn: 0.04031234607100487
iteration: 301814, loss fn: 0.09615480899810791
iteration: 301815, loss fn: 0.03339601680636406
iteration: 301816, loss fn: 0.05444253608584404
iteration: 301817, loss fn: 0.07837025076150894
iteration: 301818, loss fn: 0.04571658745408058
iteration: 301819, loss fn: 0.0754571259021759
iteration: 301820, loss fn: 0.060424864292144775
iteration: 301821, loss fn: 0.08497487753629684
iteration: 301822, loss fn: 0.04072749614715576
iteration: 301823, loss fn: 0.06285715103149414
iteration: 301824, loss fn: 0.04275471344590187
iteration: 301825, loss fn: 0.07993263006210327
iteration: 301826, loss fn: 0.05996370315551758
iteration: 301827, loss fn: 0.05727898329496384
iteration: 301828, loss fn: 0.06670845299959183
iteration: 301829, loss fn: 0.08682240545749664
iteration: 301830, loss fn: 0.0605776347219944
iteration: 301831, loss fn: 0.0998644083738327
iteration: 301832, loss fn: 0.05839413404464722
iteration: 301833, loss fn: 0.05500929057598114
iteration: 301834, loss fn: 0.04867368936538696
iteration: 301835, loss fn: 0.06332599371671677
iteration: 301836, loss fn: 0.05006019026041031
iteration: 301837, loss fn: 0.04997092857956886
iteration: 301838, loss fn: 0.05152970924973488
iteration: 301839, loss fn: 0.11623898148536682
iteration: 301840, loss fn: 0.08994917571544647
iteration: 301841, loss fn: 0.05981802940368652
iteration: 301842, loss fn: 0.05143488943576813
iteration: 301843, loss fn: 0.08891130238771439
iteration: 301844, loss fn: 0.03592237830162048
iteration: 301845, loss fn: 0.08545041084289551
iteration: 301846, loss fn: 0.02329125627875328
iteration: 301847, loss fn: 0.08399858325719833
iteration: 301848, loss fn: 0.07559352368116379
iteration: 301849, loss fn: 0.08003341406583786
iteration: 301850, loss fn: 0.07320713251829147
iteration: 301851, loss fn: 0.036291975528001785
iteration: 301852, loss fn: 0.04955410957336426
iteration: 301853, loss fn: 0.05445040762424469
iteration: 301854, loss fn: 0.06389377266168594
iteration: 301855, loss fn: 0.05571741983294487
iteration: 301856, loss fn: 0.076168954372406
iteration: 301857, loss fn: 0.023149028420448303
iteration: 301858, loss fn: 0.0526178814470768
iteration: 301859, loss fn: 0.07479213923215866
iteration: 301860, loss fn: 0.06621997803449631
iteration: 301861, loss fn: 0.02263559401035309
iteration: 301862, loss fn: 0.026334332302212715
iteration: 301863, loss fn: 0.0801561251282692
iteration: 301864, loss fn: 0.09908666461706161
iteration: 301865, loss fn: 0.03962424769997597
iteration: 301866, loss fn: 0.08364852517843246
iteration: 301867, loss fn: 0.03399571031332016
iteration: 301868, loss fn: 0.044101253151893616
iteration: 301869, loss fn: 0.05125952884554863
iteration: 301870, loss fn: 0.06218358501791954
iteration: 301871, loss fn: 0.05159570649266243
iteration: 301872, loss fn: 0.02077729068696499
iteration: 301873, loss fn: 0.06765715777873993
iteration: 301874, loss fn: 0.06384725868701935
iteration: 301875, loss fn: 0.060519397258758545
iteration: 301876, loss fn: 0.054405633360147476
iteration: 301877, loss fn: 0.0694887712597847
iteration: 301878, loss fn: 0.08208583295345306
iteration: 301879, loss fn: 0.07321281731128693
iteration: 301880, loss fn: 0.04882363975048065
iteration: 301881, loss fn: 0.07623257488012314
iteration: 301882, loss fn: 0.07702992856502533
iteration: 301883, loss fn: 0.07088446617126465
iteration: 301884, loss fn: 0.08814121782779694
iteration: 301885, loss fn: 0.05514000728726387
iteration: 301886, loss fn: 0.06723805516958237
iteration: 301887, loss fn: 0.07657888531684875
iteration: 301888, loss fn: 0.04780866950750351
iteration: 301889, loss fn: 0.050460923463106155
iteration: 301890, loss fn: 0.04763951525092125
iteration: 301891, loss fn: 0.036082372069358826
iteration: 301892, loss fn: 0.08333486318588257
iteration: 301893, loss fn: 0.08625580370426178
iteration: 301894, loss fn: 0.014729262329638004
iteration: 301895, loss fn: 0.04114793241024017
iteration: 301896, loss fn: 0.05036400258541107
iteration: 301897, loss fn: 0.06049036607146263
iteration: 301898, loss fn: 0.08495520800352097
iteration: 301899, loss fn: 0.0719871073961258
iteration: 301900, loss fn: 0.056734032928943634
iteration: 301901, loss fn: 0.05041971802711487
iteration: 301902, loss fn: 0.0733533725142479
iteration: 301903, loss fn: 0.06486395001411438
iteration: 301904, loss fn: 0.030070923268795013
iteration: 301905, loss fn: 0.07216309756040573
iteration: 301906, loss fn: 0.0865682065486908
iteration: 301907, loss fn: 0.08124643564224243
iteration: 301908, loss fn: 0.03415841981768608
iteration: 301909, loss fn: 0.04496728628873825
iteration: 301910, loss fn: 0.030917543917894363
iteration: 301911, loss fn: 0.07329483330249786
iteration: 301912, loss fn: 0.06849357485771179
iteration: 301913, loss fn: 0.05879567563533783
iteration: 301914, loss fn: 0.0770174190402031
iteration: 301915, loss fn: 0.07299403101205826
iteration: 301916, loss fn: 0.07903414964675903
iteration: 301917, loss fn: 0.048429396003484726
iteration: 301918, loss fn: 0.0636720210313797
iteration: 301919, loss fn: 0.0757187232375145
iteration: 301920, loss fn: 0.040483418852090836
iteration: 301921, loss fn: 0.04825633764266968
iteration: 301922, loss fn: 0.05141584947705269
iteration: 301923, loss fn: 0.09477432817220688
iteration: 301924, loss fn: 0.06671733409166336
iteration: 301925, loss fn: 0.06986802071332932
iteration: 301926, loss fn: 0.05960218980908394
iteration: 301927, loss fn: 0.04939144477248192
iteration: 301928, loss fn: 0.07101099193096161
iteration: 301929, loss fn: 0.023989656940102577
iteration: 301930, loss fn: 0.07740411907434464
iteration: 301931, loss fn: 0.05564786121249199
iteration: 301932, loss fn: 0.0583253838121891
iteration: 301933, loss fn: 0.0844680666923523
iteration: 301934, loss fn: 0.07783012092113495
iteration: 301935, loss fn: 0.06317044049501419
iteration: 301936, loss fn: 0.06251946091651917
iteration: 301937, loss fn: 0.06361005455255508
iteration: 301938, loss fn: 0.060395997017621994
iteration: 301939, loss fn: 0.06409545242786407
iteration: 301940, loss fn: 0.05005822703242302
iteration: 301941, loss fn: 0.0624411404132843
iteration: 301942, loss fn: 0.07084675133228302
iteration: 301943, loss fn: 0.09245997667312622
iteration: 301944, loss fn: 0.08456191420555115
iteration: 301945, loss fn: 0.045555274933576584
iteration: 301946, loss fn: 0.06969205290079117
iteration: 301947, loss fn: 0.08891106396913528
iteration: 301948, loss fn: 0.06794018298387527
iteration: 301949, loss fn: 0.03862444683909416
iteration: 301950, loss fn: 0.024247881025075912
iteration: 301951, loss fn: 0.038851503282785416
iteration: 301952, loss fn: 0.07375088334083557
iteration: 301953, loss fn: 0.021379714831709862
iteration: 301954, loss fn: 0.05469229444861412
iteration: 301955, loss fn: 0.04720935970544815
iteration: 301956, loss fn: 0.04048395901918411
iteration: 301957, loss fn: 0.06412377208471298
iteration: 301958, loss fn: 0.07819461822509766
iteration: 301959, loss fn: 0.056399110704660416
iteration: 301960, loss fn: 0.12648305296897888
iteration: 301961, loss fn: 0.09852445870637894
iteration: 301962, loss fn: 0.055493276566267014
iteration: 301963, loss fn: 0.10386167466640472
iteration: 301964, loss fn: 0.05853281915187836
iteration: 301965, loss fn: 0.055129680782556534
iteration: 301966, loss fn: 0.06736768782138824
iteration: 301967, loss fn: 0.10025985538959503
iteration: 301968, loss fn: 0.06774425506591797
iteration: 301969, loss fn: 0.035788487643003464
iteration: 301970, loss fn: 0.0495743528008461
iteration: 301971, loss fn: 0.03360556811094284
iteration: 301972, loss fn: 0.05207105353474617
iteration: 301973, loss fn: 0.03907803073525429
iteration: 301974, loss fn: 0.06956594437360764
iteration: 301975, loss fn: 0.07307038456201553
iteration: 301976, loss fn: 0.07092207670211792
iteration: 301977, loss fn: 0.05570950731635094
iteration: 301978, loss fn: 0.0693659856915474
iteration: 301979, loss fn: 0.10130751132965088
iteration: 301980, loss fn: 0.06790022552013397
iteration: 301981, loss fn: 0.08264357596635818
iteration: 301982, loss fn: 0.06024228781461716
iteration: 301983, loss fn: 0.02504201978445053
iteration: 301984, loss fn: 0.05891037732362747
iteration: 301985, loss fn: 0.04895831272006035
iteration: 301986, loss fn: 0.056914396584033966
iteration: 301987, loss fn: 0.08044852316379547
iteration: 301988, loss fn: 0.05629899725317955
iteration: 301989, loss fn: 0.03747125715017319
iteration: 301990, loss fn: 0.046556491404771805
iteration: 301991, loss fn: 0.042043689638376236
iteration: 301992, loss fn: 0.07787562906742096
iteration: 301993, loss fn: 0.060695987194776535
iteration: 301994, loss fn: 0.05759146437048912
iteration: 301995, loss fn: 0.04511547088623047
iteration: 301996, loss fn: 0.08226999640464783
iteration: 301997, loss fn: 0.08375425636768341
iteration: 301998, loss fn: 0.075889453291893
iteration: 301999, loss fn: 0.09995422512292862
